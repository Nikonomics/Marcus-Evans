SENIOR LIVING AI ANTI-PATTERN GUIDE
Protecting Operators from Expensive Mistakes

ANTI-PATTERN 1: Starting with Clinical Workflows
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"Fall prevention is our biggest problem—let's start there"
	•	"If we could predict readmissions, that would have the biggest impact"
	•	"AI could help nurses with care planning"
	•	"What about using AI to detect changes in condition?"
Why it sounds reasonable at first: Clinical outcomes are what operators care about most. Falls, readmissions, and care quality directly affect residents, families, survey results, and reputation. The logic is: "If AI is powerful, let's point it at our most important problems."
Common phrases that indicate this mistake:
	•	"Patient care comes first, so AI should start there"
	•	"Our clinical team has the most complex problems"
	•	"If we're going to invest in AI, it should impact outcomes"
	•	"Nurses are the most burned out—let's help them clinically"
WHY IT FAILS:
Root cause of failure: Clinical workflows have the highest stakes, the most variability, the strictest regulatory requirements, and the least tolerance for AI errors. They're the opposite of AI-ready—low predictability, high risk, complex judgment required.
What actually happens:
	•	AI makes predictions or recommendations with 85-90% accuracy
	•	That 10-15% error rate causes real harm—missed falls, wrong assessments
	•	Staff lose trust immediately ("It told me Mrs. Johnson was low-risk and she fell")
	•	Regulatory questions arise ("Did you let AI make clinical decisions?")
	•	Pilot gets killed, and AI loses credibility across the entire organization
	•	Years pass before anyone tries AI again
Typical timeline to failure:
	•	Week 1-2: Exciting demos, promising results
	•	Week 3-4: First errors surface
	•	Week 5-6: Staff start ignoring AI recommendations
	•	Week 8: Serious incident linked (even tangentially) to AI
	•	Week 10: Pilot terminated, AI blamed
Warning signs along the way:
	•	Clinical staff expressing skepticism about AI "understanding" their patients
	•	Discussions about "what happens if AI is wrong"
	•	Questions from compliance about documentation of AI-assisted decisions
	•	Increasing workarounds as staff bypass AI recommendations
REAL-WORLD EXAMPLE:
Scenario: A 120-bed SNF implemented an AI fall risk prediction system. The DON was excited—falls were their #1 quality issue and survey concern.
What they tried: AI analyzed gait patterns, medication changes, cognitive scores, and fall history to generate daily risk scores. Residents flagged as "high risk" were supposed to get enhanced monitoring.
What went wrong:
	•	AI flagged too many residents as high-risk (30% of census), diluting staff attention
	•	Two residents NOT flagged as high-risk fell in Week 3
	•	Staff stopped trusting the scores and reverted to their own judgment
	•	Compliance raised concerns about whether AI risk scores needed to be documented
	•	A family member asked, "Did the computer say my mom was safe to walk alone?"
Consequences:
	•	Pilot killed at Week 6
	•	$40K in software and implementation wasted
	•	Nursing staff now skeptical of all AI
	•	Administrator told corporate "AI doesn't work for us"
	•	Facilities in the network avoided AI for 2+ years
WHAT TO DO INSTEAD:
The correct approach: Start with administrative burden, not clinical decisions. Documentation, billing, scheduling, prior auth—these workflows are high-friction, high-frequency, rule-based, and have recoverable errors. Build confidence with low-stakes wins.
Why this works better:
	•	Administrative AI can be wrong without harming patients
	•	Errors are fixable (resubmit a claim, correct a note)
	•	Staff see time saved without increased risk
	•	Success builds credibility for future clinical AI
	•	The methodology—discovery, pilot, metrics—gets proven
How to transition if already on wrong path: If you've already started a clinical AI pilot and it's struggling:
	•	Pause (don't kill) the clinical pilot
	•	Run a parallel administrative AI pilot that can win quickly
	•	Let the admin win rebuild AI credibility
	•	Return to clinical with lessons learned and a proven approach
HOW CHATBOT SHOULD RESPOND:
When operator says: "We want to use AI for fall prevention / readmission prediction / clinical decision support"
What to say: "I understand why that's appealing—clinical outcomes matter most. But here's the pattern I've seen: clinical AI pilots have a high failure rate, not because the technology doesn't work, but because the stakes are too high for early experiments.
When clinical AI makes a mistake—and it will, maybe 10-15% of the time—the consequences are serious. Staff lose trust immediately, and AI loses credibility across your whole organization.
I'd suggest starting with administrative burden. Prior auth narratives, scheduling, documentation—these are high-friction, rule-based, and forgiving of errors. Build confidence with a low-stakes win, prove the methodology, and THEN bring that discipline to clinical workflows.
What's the biggest administrative headache in your building right now? Let's start there."
Framework to apply: TRUST (specifically the Risk Tolerance dimension), 5 Attributes of AI-Ready Workflows (predictability and risk)

ANTI-PATTERN 2: IT-Led AI Initiatives
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"Our IT director is excited and wants to lead the AI initiative"
	•	"IT handles all our technology, so they should handle AI"
	•	"IT already works with vendors, so they'll evaluate AI tools"
	•	"We'll let IT pick the platform and then train operations"
Why it sounds reasonable at first: AI is technology. IT manages technology. IT has vendor relationships, security expertise, and system integration knowledge. It's a natural fit—on paper.
Common phrases that indicate this mistake:
	•	"IT will identify the best solution"
	•	"We're waiting for IT to evaluate platforms"
	•	"IT is building the requirements document"
	•	"Operations will be trained once IT finishes setup"
WHY IT FAILS:
Root cause of failure: IT doesn't feel the pain. They don't live in PCC. They don't know which BOM is drowning in spreadsheets. They don't understand why prior auth takes 3 hours or why call-off coverage generates 150 texts. They optimize for technology, not for workflow.
What actually happens:
	•	IT evaluates AI platforms based on technical criteria (integration, security, scalability)
	•	They select a solution that checks all the IT boxes
	•	The solution gets deployed to operations with minimal workflow mapping
	•	Staff discover the tool adds steps to their process—even just one extra click
	•	Adoption crashes because operations wasn't involved in selection
	•	The AI becomes "shelfware"—technically deployed, never used
	•	Leadership concludes "AI didn't work for us"
Typical timeline to failure:
	•	Month 1-3: IT evaluation process
	•	Month 4-6: Procurement and security review
	•	Month 7-9: Technical implementation
	•	Month 10-12: "Training" operations on the new system
	•	Month 12+: Adoption fails, finger-pointing begins
Warning signs along the way:
	•	Operations not involved in vendor demos
	•	Requirements document focused on technical specs, not workflow outcomes
	•	No shadowing of actual workflow before implementation
	•	Success metrics are technical (uptime, integration) not operational (time saved)
	•	Project timeline doesn't include workflow mapping
REAL-WORLD EXAMPLE:
Scenario: A regional SNF operator with 15 facilities let their IT department lead an AI documentation initiative. IT was enthusiastic and thorough.
What they tried: IT ran a 6-month evaluation of AI documentation platforms, scored vendors on integration capabilities, security compliance, and scalability. They selected a enterprise solution and spent 4 months on implementation.
What went wrong:
	•	The tool required nurses to use a separate interface (not integrated into their EHR workflow)
	•	Documentation "assistance" required clicking to a different screen, typing prompts, then copying back
	•	The process added 2-3 minutes per note—exactly the opposite of the goal
	•	Nurses stopped using it within 3 weeks
	•	IT blamed training; nursing blamed the tool; leadership blamed both
Consequences:
	•	$150K+ in platform licensing (3-year contract)
	•	10 months of IT time
	•	Zero adoption
	•	Deep organizational distrust between IT and operations
	•	The phrase "remember when IT tried to do AI documentation" became shorthand for failed initiatives
WHAT TO DO INSTEAD:
The correct approach: Ops-led, IT-enabled. Operations identifies the workflow, owns the pilot, measures success, and drives adoption. IT provides security review, integration support, access management, and system stability.
Think of it as: 80% workflow, 20% technology.
Why this works better:
	•	The people who feel the pain pick the solution
	•	Workflow mapping happens before vendor selection
	•	Success is defined operationally ("45 minutes saved per admission")
	•	Champions are in the building, not in the server room
	•	Adoption is built in because users shaped the solution
How to transition if already on wrong path: If IT is already leading:
	•	Don't take it away—add an operational co-owner
	•	Insist that any AI pilot has an operational champion who owns success metrics
	•	Require workflow shadowing before any implementation
	•	Add operational success criteria to IT's technical evaluation
HOW CHATBOT SHOULD RESPOND:
When operator says: "Our IT team is taking the lead on AI" or "IT is evaluating AI platforms for us"
What to say: "That's great that IT is engaged—you'll need them. But here's the pattern that typically fails:
When AI is led by IT alone, it becomes a technology project instead of a workflow project. IT doesn't feel the pain of prior auth denials. They don't live in PCC. They don't manage the staffing matrix or sit through survey prep.
The model that works: Ops leads, IT enables.
Operations identifies the workflow, owns the pilot, and measures success. IT ensures security, manages access, and supports integration.
My suggestion: Keep IT involved as a partner, but find an operational champion—a DON, BOM, or administrator—to own the first pilot. Make sure the success metrics are operational ('45 minutes saved per admission'), not technical ('99.9% uptime').
Who in operations is feeling the most pain right now? That's your pilot owner."
Framework to apply: "Where AI Should Live" (Ops-Led + IT-Enabled)

ANTI-PATTERN 3: Multi-Building Pilots
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We'll pilot at three facilities to get better data"
	•	"Let's test in different markets to see what works"
	•	"Our board wants to see results across the organization"
	•	"One building isn't statistically significant"
Why it sounds reasonable at first: More data points seem better. If you only pilot one building, maybe it's an anomaly. Multiple buildings give you confidence the results will scale. Executives want to see organizational impact, not a single-facility experiment.
Common phrases that indicate this mistake:
	•	"We need diverse data to know if this works"
	•	"Our buildings are all different, so we need to test in each type"
	•	"Corporate wants a company-wide view"
	•	"We can't justify the investment for just one facility"
WHY IT FAILS:
Root cause of failure: Complexity explodes. Three buildings means three sets of staff to train, three variations of "the same" workflow, three schedules to coordinate, three potential points of failure. When something goes wrong (and something will), you can't tell if it's the tool, the building, the training, or the workflow.
What actually happens:
	•	Pilot launches in 3 buildings simultaneously
	•	Each building does the workflow slightly differently (turns out they weren't identical)
	•	Building A loves it; Building B has problems; Building C doesn't engage at all
	•	Results are inconsistent—leadership can't tell if AI works or not
	•	Troubleshooting is impossible because you can't isolate variables
	•	Pilot drags on inconclusively
	•	No clear decision: scale, pivot, or kill?
Typical timeline to failure:
	•	Week 1-2: Launches go okay in some buildings, rocky in others
	•	Week 3-4: Building B is struggling; you can't figure out why
	•	Week 5-6: Metrics are all over the place
	•	Week 8: You realize each building does the workflow differently
	•	Week 12: Inconclusive results; pilot extended indefinitely
	•	Week 16+: Pilot quietly abandoned with no clear lessons learned
Warning signs along the way:
	•	Different buildings report dramatically different results
	•	Project manager spending more time coordinating than improving
	•	Each building requesting customizations
	•	Impossible to have one weekly review meeting that's relevant for all sites
	•	No one can answer "Is this working?"
REAL-WORLD EXAMPLE:
Scenario: A 12-facility operator decided to pilot AI scheduling assistance in 4 buildings—one urban SNF, one rural ALF, one suburban SNF, and one memory care.
What they tried: Same AI scheduling tool deployed to all four simultaneously. Weekly corporate check-ins to review progress.
What went wrong:
	•	Each building had different scheduling processes (one used Excel, one used the EHR, two used different third-party tools)
	•	Agency usage patterns were completely different by market
	•	The urban SNF had union considerations the others didn't
	•	Staff turnover during the pilot ranged from 5% to 30% across buildings
	•	Results were impossible to compare—what worked in one building failed in another
	•	Corporate couldn't determine if the tool was good or if implementation varied
Consequences:
	•	4-month pilot extended to 8 months with no conclusion
	•	Executive sponsor lost patience and moved to other priorities
	•	Massive consultant spend on "analyzing inconsistent results"
	•	The phrase "we tried that" blocked scheduling AI discussions for years
	•	Each building blamed the others for the failure
WHAT TO DO INSTEAD:
The correct approach: One building. One workflow. One team.
Pick your best building—the one with the most engaged administrator, the clearest version of the workflow, the most stable staff. Prove it works there. Then scale with confidence.
Why this works better:
	•	When something goes wrong, you can identify the cause
	•	Weekly reviews are focused and actionable
	•	Success is unambiguous
	•	You learn what works before scaling complexity
	•	Your first building becomes your proof point and your training center
How to transition if already on wrong path: If you're already piloting across multiple buildings:
	•	Pick the building that's doing best
	•	Focus all resources there for 4 weeks
	•	Get a clear win (or clear failure) at that one site
	•	Use that to inform what to do at other sites
	•	Roll to other buildings sequentially, not simultaneously
HOW CHATBOT SHOULD RESPOND:
When operator says: "We want to pilot at 3 buildings to get more data" or "Let's test in different facilities"
What to say: "I understand the instinct—more data seems better. But here's what typically happens:
Three buildings means three sets of staff, three variations of the workflow, three potential failure points. When something goes wrong—and it will—you can't tell if it's the tool, the building, the training, or the workflow.
Multi-building pilots almost always produce inconclusive results. Building A loves it, Building B struggles, Building C doesn't engage. You end up with no clear answer and no clear path forward.
Start with one building. One workflow. One team.
Pick your best building—most engaged administrator, clearest workflow, most stable staff. Prove it works there. Then you have a proof point AND a training center for the rollout.
Which building has the most engaged administrator and the best chance of success?"
Framework to apply: Pilot methodology (ONE building, ONE workflow, ONE team)

ANTI-PATTERN 4: Vendor-First Approach
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We're evaluating three AI vendors this month"
	•	"A vendor showed us a demo and it looked amazing"
	•	"We're trying to find the best AI platform"
	•	"Let's see what's available and then figure out where to use it"
Why it sounds reasonable at first: AI is complex—let the experts guide you. Vendors know their products and can show you possibilities you hadn't considered. Getting demos is educational. And if you don't know what's out there, how can you pick the right solution?
Common phrases that indicate this mistake:
	•	"The vendor said their tool can do everything"
	•	"We're waiting to see what features are available"
	•	"The demo was really impressive"
	•	"They said facilities like ours are using it"
WHY IT FAILS:
Root cause of failure: You're looking at solutions before defining problems. Vendors show you their capabilities, not your priorities. You get excited about features that don't address your actual pain. You end up buying a tool that can do 20 things, none of which is the specific thing you needed most.
What actually happens:
	•	Vendor shows impressive demo with 15 features
	•	You think, "We could use all of these!"
	•	You sign a contract without identifying your #1 priority workflow
	•	Implementation begins, but you're spread across too many use cases
	•	Nothing works well because nothing was implemented deeply
	•	Staff see AI as "another system to learn" rather than a solution to their pain
	•	Tool underperforms because it wasn't matched to your most painful workflow
Typical timeline to failure:
	•	Month 1: Vendor demos (exciting!)
	•	Month 2: Evaluation and selection
	•	Month 3: Contract signed
	•	Month 4-6: Implementation of "everything"
	•	Month 7-9: Staff complaining, adoption lagging
	•	Month 12: Renewal comes up; hard to justify ROI
Warning signs along the way:
	•	Pilot plan includes 5+ features/workflows
	•	No single workflow owner identified
	•	Success metrics are vague ("improve efficiency")
	•	Vendor doing most of the talking about what you should do
	•	Excitement about capabilities rather than specific pain points
REAL-WORLD EXAMPLE:
Scenario: A 200-bed SNF watched demos from three AI vendors. One offered documentation, billing, scheduling, and analytics—an "all-in-one" platform. The CFO liked the consolidated approach.
What they tried: Bought the all-in-one platform. Implementation plan included launching documentation assistance, billing automation, and scheduling optimization in the first 6 months.
What went wrong:
	•	No single workflow was prioritized; resources split across three initiatives
	•	Documentation module required workflow changes nurses weren't prepared for
	•	Billing automation conflicted with existing processes the BOM had refined over years
	•	Scheduling feature didn't integrate with their preferred staffing agency
	•	After 6 months, all three were "partially implemented" but none was actually working
	•	Staff didn't know which feature to focus on; defaulted to old methods
Consequences:
	•	$120K annual contract with minimal ROI demonstrated
	•	Staff overwhelmed and frustrated ("They keep adding systems")
	•	Vendor blamed the facility; facility blamed the vendor
	•	No clear success story to justify renewal
	•	Organization reluctant to try AI again with any vendor
WHAT TO DO INSTEAD:
The correct approach: Problem-first, then vendor.
	•	Identify your highest-pain workflow using discovery methods
	•	Score it against the 5 Attributes
	•	Define what success looks like (specific metrics)
	•	THEN find vendors who solve that specific problem
	•	Evaluate them against your defined needs, not their feature lists
Why this works better:
	•	You know exactly what you need before you see any demos
	•	You can ask vendors: "How specifically does your tool handle [this exact workflow]?"
	•	You avoid being distracted by features you don't need
	•	Implementation is focused on ONE proven win
	•	ROI is measurable because you defined metrics before buying
How to transition if already on wrong path: If you've already bought a vendor-first solution:
	•	Stop trying to use all the features
	•	Pick ONE feature that addresses your highest-pain workflow
	•	Implement that ONE feature deeply and measure results
	•	Only expand to other features after proving the first one works
HOW CHATBOT SHOULD RESPOND:
When operator says: "We're evaluating vendors" or "A vendor showed us a demo" or "We're trying to pick an AI platform"
What to say: "Before you evaluate any vendors, I have a critical question: Which specific workflow are you trying to fix?
The biggest mistake I see is picking tools before picking workflows. Vendor demos show you 20 features, but which one solves the problem that's actually causing pain in your buildings?
Here's my suggestion:
	•	First, identify your #1 pain point workflow
	•	Score it: Is it high-friction? High-frequency? Predictable?
	•	Define what 'success' looks like (time saved, errors reduced)
	•	THEN look at vendors who solve that specific problem
When you do vendor demos, ask: 'How specifically do you handle [exact workflow]?' If they pivot to features you didn't ask about, that's a red flag.
What's the specific workflow you're hoping AI will fix?"
Framework to apply: 5 Attributes, Discovery process (workflow-first)

ANTI-PATTERN 5: Skipping Workflow Mapping
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We know our process—we don't need to document it"
	•	"Our team has been doing this for years"
	•	"Let's just start and figure it out as we go"
	•	"The vendor will handle implementation"
Why it sounds reasonable at first: Your staff do this workflow every day. They know it intimately. Documenting it seems like busy work—time that could be spent actually implementing. And the vendor has seen this process at other facilities, right?
Common phrases that indicate this mistake:
	•	"Everyone knows how this works"
	•	"We don't have time for documentation"
	•	"It's pretty straightforward"
	•	"The vendor said they don't need workflow details"
WHY IT FAILS:
Root cause of failure: The workflow people think they do is not the workflow they actually do. The "official" process in the policy manual has been adapted, worked around, and modified over years. The person who does it fastest does it differently than the person who does it by the book. AI implemented for the policy workflow fails against the real workflow.
What actually happens:
	•	Team describes the "official" process to vendor
	•	Vendor builds AI to support that process
	•	AI launches, and staff immediately find it doesn't fit how they actually work
	•	Staff create workarounds to do their real process AND use the AI system
	•	AI adds steps instead of removing them
	•	Adoption dies because the tool fights the workflow
Typical timeline to failure:
	•	Week 1-2: Launch seems okay
	•	Week 3: Staff start saying "this doesn't work for how I do it"
	•	Week 4-5: Workarounds emerge
	•	Week 6-8: Staff revert to old methods, use AI only when required
	•	Week 10+: AI abandoned; declared "not a good fit"
Warning signs along the way:
	•	Implementation plan based on policy manual, not observed workflow
	•	No shadowing of actual staff doing actual work
	•	Different staff members describe the same process differently
	•	Vendor hasn't asked to observe the workflow firsthand
	•	The "hero" who does the workflow best wasn't interviewed
REAL-WORLD EXAMPLE:
Scenario: A facility implemented AI-assisted prior auth to reduce documentation time. The policy said nurses should document throughout the day and BOMs should pull from the EHR.
What they tried: AI was configured to extract from structured EHR fields and generate prior auth narratives based on the policy process.
What went wrong:
	•	In reality, nurses documented in "batch mode" at shift end, not throughout the day
	•	The most relevant clinical details were in free-text notes, not structured fields
	•	BOMs had developed a personal "template" with specific payer requirements
	•	AI missed critical information and generated narratives that needed 80% rewriting
	•	The tool added time because BOMs had to check AI's work and then fix it
Consequences:
	•	BOM loudly rejected the tool: "This makes my job harder, not easier"
	•	Prior auth backlog actually increased during pilot
	•	Denial rate went up due to incomplete narratives during transition
	•	Pilot killed at Week 4
	•	Vendor blamed the facility's "non-standard processes"
WHAT TO DO INSTEAD:
The correct approach: Shadow the hero. Document reality.
Weeks 2-3 of any pilot should be spent:
	•	Shadowing the person who does the workflow fastest/best
	•	Recording their screen (with Loom or phone)
	•	Capturing every click, every copy/paste, every workaround
	•	Documenting the REAL process, not the policy process
Why this works better:
	•	You implement AI for how work actually happens
	•	The "hero" feels heard and becomes your champion
	•	You discover friction points you didn't know existed
	•	AI removes actual steps instead of fighting the workflow
	•	Adoption succeeds because the tool fits reality
How to transition if already on wrong path: If you've already launched without workflow mapping:
	•	Pause the pilot
	•	Spend 2-4 hours shadowing your best user
	•	Document every deviation between AI's assumptions and reality
	•	Reconfigure the tool (or accept it doesn't fit)
	•	Restart with realistic expectations
HOW CHATBOT SHOULD RESPOND:
When operator says: "We know our process—we don't need to map it" or "Let's just get started"
What to say: "I've seen this pattern before, and it usually doesn't end well. Here's why:
The workflow people think they do is rarely the workflow they actually do. The policy version has been adapted over years. The person who does it fastest does it differently than the person who does it by the book.
If you implement AI for the policy workflow, it will fail against the real workflow. Staff will say 'this doesn't work for how I actually do it' and create workarounds. AI ends up adding steps instead of removing them.
Shadow your hero. Find the person who does this workflow best and watch them for 2 hours. Use Loom or your phone to record their screen. Capture every click, every copy/paste, every 'Ugh, hold on...'
This is where the gold lives. Document reality, not policy.
Who's your hero for this workflow—the person who makes it happen despite the friction?"
Framework to apply: Workflow documentation methodology, "Hero-centered design"

ANTI-PATTERN 6: No Success Metrics Defined
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We'll know it's working when we see it"
	•	"It's hard to measure efficiency—it's qualitative"
	•	"We're just trying to see if this helps"
	•	"Let's pilot it and then figure out if it worked"
Why it sounds reasonable at first: AI is new. You're not sure what's possible. It seems premature to commit to specific numbers before you've even started. You want flexibility to define success based on what actually happens.
Common phrases that indicate this mistake:
	•	"We want to improve efficiency" (without defining how much)
	•	"Staff satisfaction will improve" (without measuring baseline)
	•	"We're testing the concept"
	•	"ROI is hard to calculate until after we try it"
WHY IT FAILS:
Root cause of failure: Without defined metrics, you can't prove value. At the end of the pilot, someone will ask "Did this work?" and you'll have opinions, not data. Opinions don't get budget renewals or scale approvals. Worse, without metrics, you can't course-correct during the pilot—you don't know if you're on track.
What actually happens:
	•	Pilot launches with vague goals
	•	Some staff say it's great; others say it's not
	•	Sponsor asks "Is this working?" and gets conflicting answers
	•	At pilot end, no objective data to prove value
	•	CFO asks "What's the ROI?" and there's no answer
	•	Pilot either dies quietly or drags on without decision
	•	Organization concludes "We couldn't tell if it worked"
Typical timeline to failure:
	•	Week 1-4: Pilot runs without clear metrics
	•	Week 5-8: Anecdotal feedback (positive and negative)
	•	Week 10-12: Pilot end; no objective success criteria
	•	Week 12+: "Extended pilot" because no one knows how to decide
	•	Month 6+: Pilot dies quietly; no lessons learned
Warning signs along the way:
	•	No baseline measurement before pilot started
	•	Weekly reviews discuss feelings, not numbers
	•	Different stakeholders have different definitions of success
	•	CFO/sponsor hasn't bought into the metrics
	•	Can't answer "How much time are we saving?"
REAL-WORLD EXAMPLE:
Scenario: A facility piloted AI meeting summarization for their IDT meetings. Goal: "improve documentation efficiency."
What they tried: Deployed Otter.ai for all IDT meetings. Staff seemed to like it.
What went wrong:
	•	No one measured how long IDT documentation took before the pilot
	•	No one measured how long it took after
	•	After 8 weeks, Administrator asked "Should we keep paying for this?"
	•	Some nurses said "It's helpful." Others said "I don't use the summaries."
	•	No data on time saved, errors reduced, or adoption rate
	•	CFO rejected renewal: "I can't justify the cost with just 'it's helpful'"
Consequences:
	•	Tool discontinued despite staff generally liking it
	•	Opportunity cost: a clear win could have built momentum for more AI
	•	Organization learned wrong lesson: "Even when AI works, we can't prove it"
	•	Future AI proposals required "guaranteed ROI"—an impossible bar
WHAT TO DO INSTEAD:
The correct approach: Define success in Week 1. Measure baseline before starting.
For any pilot, define:
	•	Primary metric: What's the one number that must improve? (time, errors, cost)
	•	Baseline: What's that number today? (measure before pilot starts)
	•	Target: What improvement would make this a success? (be conservative)
	•	Secondary metrics: What else might improve? (track but don't obsess)
Why this works better:
	•	You can answer "Is this working?" at any point
	•	CFO has data to approve continued investment
	•	You can course-correct during the pilot if metrics aren't improving
	•	Success is objective, not political
	•	Your win is undeniable and shareable
How to transition if already on wrong path: If you've already started without metrics:
	•	Stop and define metrics now (better late than never)
	•	Measure current state even though you're mid-pilot
	•	Commit to 4 more weeks with clear measurement
	•	Acknowledge baseline isn't perfect but proceed anyway
HOW CHATBOT SHOULD RESPOND:
When operator says: "We'll know if it's working when we see it" or "It's hard to measure efficiency"
What to say: "I've seen pilots fail at the finish line because they couldn't answer one question: 'Did this work?'
Without defined metrics, you'll have opinions but not data. At the end of the pilot, some staff will say it's great, others will say it's not. Your CFO will ask about ROI and you won't have an answer.
Define success in Week 1. Measure baseline before you start.
For scheduling AI, for example:
	•	How long does each call-off take to resolve today? (Measure 10 and average)
	•	How many texts/calls per shift fill? (Track for one week)
	•	What's your current agency vs. internal fill rate?
Then set your target: 'We'll consider this successful if we reduce resolution time by 30% and cut texts-per-fill by half.'
At the end of 60 days, you'll have an undeniable answer.
What metrics would matter most to your CFO?"
Framework to apply: Metrics framework (time, touches, errors, satisfaction)

ANTI-PATTERN 7: Over-Customization
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We have unique processes that need customization"
	•	"The standard version won't work for our payers"
	•	"Can we add a field for [specific thing]?"
	•	"Our buildings do this differently, so we need variations"
Why it sounds reasonable at first: Every facility IS different. You have specific payer relationships, unique EHR configurations, state-specific regulations. It seems logical that AI should adapt to your reality, not force you to adapt to its defaults.
Common phrases that indicate this mistake:
	•	"We need it customized for [state/payer/building type]"
	•	"Can the vendor build a specific feature for us?"
	•	"Our workflow has steps they don't cover"
	•	"We'll pay extra for customization"
WHY IT FAILS:
Root cause of failure: Customization is expensive, slow, and fragile. Every custom feature needs to be built, tested, maintained, and updated separately from the core product. You end up with a brittle system that breaks when the vendor updates their platform, and costs 3x as much to maintain.
What actually happens:
	•	Operator requests 5-10 customizations before launch
	•	Vendor agrees (for a fee) and timeline extends 2-3 months
	•	Pilot finally launches with custom features
	•	Vendor releases platform update; custom features break
	•	Operator waits weeks for fixes while staff workaround
	•	Over time, customizations create technical debt
	•	Costs escalate; ROI never materializes
Typical timeline to failure:
	•	Month 1-2: Customization requirements gathered
	•	Month 3-5: Custom development
	•	Month 6: Finally launch
	•	Month 8: First platform update breaks custom features
	•	Month 10: Frustration builds; considering replacement
	•	Year 2: Paying premium support for custom maintenance
Warning signs along the way:
	•	Vendor quoting custom development fees
	•	Implementation timeline pushed for "custom builds"
	•	Multiple rounds of "just one more customization"
	•	Core functionality working but waiting on custom features
	•	Vendor's standard users getting updates you're not getting
REAL-WORLD EXAMPLE:
Scenario: A 20-facility operator chose an AI documentation platform but requested extensive customizations: custom templates per building, state-specific terminology, integration with their proprietary reporting system, and custom approval workflows.
What they tried: Paid $50K in custom development on top of licensing. Implementation took 8 months instead of 2.
What went wrong:
	•	Vendor's platform update (v3.0) broke two custom features
	•	Fix took 6 weeks; staff reverted to old methods
	•	Custom reporting integration never worked properly
	•	Buildings with custom templates couldn't share best practices
	•	Upgrade to v4.0 required $30K in recustomization
Consequences:
	•	Total spend: $200K+ over 3 years (vs. $80K standard)
	•	ROI was negative due to custom costs
	•	Stuck on outdated version while standard users got improvements
	•	Massive switching cost trapped them with a broken implementation
	•	Vendor relationship became adversarial over support obligations
WHAT TO DO INSTEAD:
The correct approach: Use the 80% solution. Accept that an off-the-shelf tool won't fit perfectly. Adapt your workflow to the tool when possible. Customize only when absolutely necessary and only after proving the standard version works.
Why this works better:
	•	Faster implementation (weeks, not months)
	•	Lower cost (no custom development fees)
	•	Better support (vendor knows standard product best)
	•	Continuous improvement (you get platform updates automatically)
	•	Easier rollout (standard training materials work)
How to transition if already on wrong path: If you've already over-customized:
	•	Inventory all customizations
	•	Identify which are truly essential vs. nice-to-have
	•	Ask vendor about moving to standard features
	•	Phase out customizations over 12 months
	•	Accept temporary gaps in exchange for long-term stability
HOW CHATBOT SHOULD RESPOND:
When operator says: "We need customization" or "Can the vendor build this feature for us?"
What to say: "I'd pump the brakes on customization. Here's the pattern I've seen:
Customizations are expensive, slow, and fragile. They extend implementation by months. When the vendor updates their platform, custom features break. Over time, you're paying premium support for custom maintenance while standard users get improvements.
Use the 80% solution. If an off-the-shelf tool fits 80% of your workflow, adapt your workflow to close the gap. Customize only when absolutely necessary—and only after proving the standard version works.
Ask yourself: Is this customization truly essential, or is it 'how we've always done it'? Many 'unique' processes are actually just habits that could be updated.
If you must customize, do it in Phase 2—after you've proven the standard version delivers value. Never customize before you've had a win.
What specific customization feels essential? Let's evaluate whether it's truly necessary."
Framework to apply: Pilot methodology (simple first, customize later)

ANTI-PATTERN 8: Piloting Multiple Workflows Simultaneously
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We're piloting AI for documentation, scheduling, AND billing"
	•	"We want to get a broad view of AI impact"
	•	"Different departments have different priorities, so we're doing all of them"
	•	"If we're going to invest in AI, let's maximize the impact"
Why it sounds reasonable at first: If AI is good, more AI is better, right? Multiple pilots mean faster learning, broader impact, and something for everyone. It seems efficient to test multiple use cases simultaneously.
Common phrases that indicate this mistake:
	•	"Let's try everything and see what sticks"
	•	"Each department wants their own pilot"
	•	"We're spreading the investment across initiatives"
	•	"This way we'll know which AI use case works best for us"
WHY IT FAILS:
Root cause of failure: Attention is the scarcest resource. Multiple simultaneous pilots mean divided focus—for the project sponsor, for training, for IT support, for vendor engagement. Nothing gets deep attention. Results are mediocre across the board. You learn nothing about what works because nothing was done well.
What actually happens:
	•	Three pilots launch in Month 1
	•	Each pilot gets 33% of attention, resources, and support
	•	All three struggle with adoption (normal early challenges)
	•	Team can't focus on fixing any one because they're managing three
	•	All three underperform; none gets the support to succeed
	•	At pilot end, all three have middling results
	•	Organization concludes "AI has potential but didn't quite work"
Typical timeline to failure:
	•	Week 1-2: All three launch (exciting!)
	•	Week 3-4: All three have early issues (normal)
	•	Week 5-6: Team scrambles across all three; can't focus on any
	•	Week 8-10: One pilot is ignored in favor of two others
	•	Week 12: Inconsistent results across all three
	•	Month 4: No clear success to celebrate or scale
Warning signs along the way:
	•	Project meetings jump between unrelated workflows
	•	Different success metrics for each pilot (impossible to compare)
	•	Sponsor can't articulate which pilot is priority
	•	When one pilot struggles, resources shift to another instead of fixing
	•	Staff confused about what they're supposed to use
REAL-WORLD EXAMPLE:
Scenario: An operator decided to pilot AI in three areas: nurse documentation (Clinical), prior auth (Billing), and staff scheduling (HR). Three departments, three vendors, three pilots—all launching in Q1.
What they tried: Each department got their own AI tool and their own success metrics. Monthly executive review would compare progress.
What went wrong:
	•	Nurse documentation pilot hit early adoption challenges—nurses needed hands-on support
	•	Prior auth pilot needed workflow mapping that was never completed
	•	Scheduling pilot launched but the champion was pulled to help with documentation
	•	No pilot got deep enough attention to succeed
	•	At Month 3 executive review: all three were "in progress" with no clear wins
	•	Executive sponsor frustrated: "We invested in three AI projects and can't point to a single success"
Consequences:
	•	All three pilots eventually discontinued
	•	$180K total spent across three vendors
	•	Zero proof points for future AI investment
	•	Organization declared "AI-fatigued"
	•	Departments blamed each other for the failure
WHAT TO DO INSTEAD:
The correct approach: Sequential pilots, not simultaneous. One workflow. One building. One team. Prove it works. Celebrate the win. Then—and only then—move to the next workflow.
Why this works better:
	•	All attention on one pilot means it gets what it needs to succeed
	•	When issues arise, you can fix them (not pivot to another pilot)
	•	Success is clear and celebrated before moving on
	•	Lessons learned from Pilot 1 improve Pilot 2
	•	Momentum builds with each sequential win
How to transition if already on wrong path: If you're already running multiple pilots:
	•	Pick the one with the best chance of success
	•	Pause (don't kill) the others
	•	Give the priority pilot 100% focus for 30 days
	•	Get a clear win before resuming other pilots
	•	Roll the wins sequentially
HOW CHATBOT SHOULD RESPOND:
When operator says: "We're doing three pilots" or "Different departments want their own AI initiatives"
What to say: "I strongly recommend against simultaneous pilots. Here's the pattern:
Attention is your scarcest resource. Three pilots mean divided focus—for training, support, troubleshooting. All three get 33% of what they need. All three underperform. At the end, you have three mediocre results and no clear success story.
One workflow. One building. One team. Prove it works. Celebrate the win. Then move to the next.
Sequential pilots build momentum. Each success funds the next. Lessons from Pilot 1 make Pilot 2 better.
If you're already running multiple pilots, here's what I'd do:
	•	Pick the one with the best chance of success
	•	Pause (don't kill) the others
	•	Give your priority pilot 100% focus for 30 days
	•	Get a clear win, then resume the others
Which of your current pilots has the strongest champion and the clearest path to success?"
Framework to apply: "One at a time" principle, sequential methodology

ANTI-PATTERN 9: "Transform Everything" Approach
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We need an AI transformation strategy"
	•	"Our CEO wants to 'become an AI-powered organization'"
	•	"We're developing a comprehensive AI roadmap"
	•	"Let's think big—what could AI do across our entire operation?"
Why it sounds reasonable at first: AI is revolutionary technology. Why think small? Big visions attract investment, energize teams, and position you as innovative. A comprehensive strategy seems more professional than "fixing one spreadsheet workflow."
Common phrases that indicate this mistake:
	•	"AI will transform how we operate"
	•	"We need an enterprise AI strategy"
	•	"This is bigger than one workflow"
	•	"Our board expects a vision, not a pilot"
WHY IT FAILS:
Root cause of failure: "Transform everything" paralyzes action. When everything is a priority, nothing is a priority. The scope is so large that no one knows where to start. Planning takes months. Quick wins never happen. Change management becomes impossible because you're changing too much at once.
What actually happens:
	•	Organization spends 6 months developing "AI transformation roadmap"
	•	Roadmap identifies 30+ AI opportunities across all departments
	•	Implementation team overwhelmed; doesn't know where to start
	•	First projects are ambitious and complex (because "transformation")
	•	First projects struggle due to scope and change management
	•	Momentum dies before any quick wins build confidence
	•	"Transformation" becomes "transformation fatigue"
Typical timeline to failure:
	•	Month 1-6: Strategy development, consultant engagement, roadmap creation
	•	Month 7-9: Attempts to launch "Phase 1" (still too broad)
	•	Month 10-12: First initiatives stall; scope reduction discussions
	•	Year 2: Quietly abandon "transformation" for smaller initiatives
	•	Year 2+: Organization becomes cautious about big AI claims
Warning signs along the way:
	•	Strategy document but no pilot running
	•	Roadmap with 20+ initiatives and no clear first step
	•	Consultants talking more than operators
	•	"Future state" discussions without "current state" documentation
	•	Executive presentations without frontline staff involvement
REAL-WORLD EXAMPLE:
Scenario: A regional operator hired a consulting firm to develop their "AI Transformation Strategy." The firm spent 5 months creating a beautiful 80-page roadmap.
What they tried: The roadmap identified 25 AI opportunities across clinical, operational, and financial domains. Phase 1 included 8 initiatives to "establish the foundation."
What went wrong:
	•	No one could prioritize among the 8 Phase 1 initiatives
	•	Resources spread thin; all 8 made slow progress
	•	After 12 months, no single initiative was "done" and working
	•	Staff experienced "initiative fatigue"—too many new things, none working well
	•	Board asked "What have we achieved?" Answer: "We're still building the foundation"
	•	CEO who championed transformation moved to new role; successor deprioritized
Consequences:
	•	$400K spent on consulting and Year 1 initiatives
	•	Zero working AI implementations after 18 months
	•	Organization more skeptical of AI than before
	•	New leadership instituted "no more big bets" policy
	•	Transformation became cautionary tale internally
WHAT TO DO INSTEAD:
The correct approach: One win, then scale. Pick one workflow. Get it working. Celebrate. Document. Then expand. Vision follows proof, not the reverse.
Why this works better:
	•	Quick wins build credibility for larger initiatives
	•	Staff see AI actually working before being asked to "transform"
	•	Methodology gets proven before scaling complexity
	•	Resources stay focused; results are clear
	•	"Transformation" emerges from accumulated wins, not top-down mandate
How to transition if already on wrong path: If you're stuck in transformation planning:
	•	Shelve the roadmap (don't abandon, shelve)
	•	Pick ONE workflow from the roadmap's recommendations
	•	Pilot it in 60 days with clear metrics
	•	Celebrate the win publicly
	•	Let momentum from real success drive next priorities
HOW CHATBOT SHOULD RESPOND:
When operator says: "We need an AI transformation strategy" or "Our CEO wants us to become AI-powered"
What to say: "I'd challenge the framing of 'transformation.'
When everything is a priority, nothing is a priority. 'Transform everything' strategies typically produce 6 months of planning, 18 months of mediocre execution, and zero clear wins to point to.
Vision follows proof, not the reverse.
The organizations that are actually becoming 'AI-powered' didn't start with transformation strategies. They started with one frustrated person and one painful workflow. They got a win. Celebrated it. Documented it. Then expanded.
Here's my suggestion: Shelve the transformation roadmap for now. Pick ONE workflow—the most painful, most obvious opportunity. Pilot it in 60 days. Get a win your team can point to.
That first win becomes your proof point. It builds credibility for the next initiative. And the next. A year from now, you'll have transformed—not because you planned to, but because you accumulated wins.
What's the single most painful workflow in your organization right now?"
Framework to apply: "One win, then scale" principle, $100K Rule for prioritization

ANTI-PATTERN 10: Ignoring the Workflow Hero
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"IT and the vendor will implement it, then we'll train the team"
	•	"We don't want to burden our staff during implementation"
	•	"Our scheduler is too busy—we'll just show her the new system when it's ready"
	•	"The department head approved it; the staff will adapt"
Why it sounds reasonable at first: Staff are overwhelmed. Adding implementation meetings to their workload seems cruel. Better to let the experts build it right, then present the finished product. You're protecting your people by keeping them out of the messy development process.
Common phrases that indicate this mistake:
	•	"We'll train them when it's ready"
	•	"They're too busy to be involved right now"
	•	"Management approved it—they'll have to adapt"
	•	"The vendor said they handle implementation"
WHY IT FAILS:
Root cause of failure: The "Workflow Hero"—the scheduler with 14 spreadsheets, the BOM who knows every payer rule, the MDS nurse who catches every error—holds institutional knowledge that doesn't exist anywhere else. Implement without them and you'll miss critical nuances. They also hold adoption keys: if the hero rejects the tool, everyone follows.
What actually happens:
	•	System implemented based on policy, not practice
	•	Hero encounters it for the first time during "training"
	•	Hero immediately identifies problems: "This isn't how we actually do it"
	•	Hero feels excluded and unheard
	•	Hero complains to peers; skepticism spreads
	•	Hero creates workarounds; adoption stalls
	•	Hero's resistance becomes passive (or active) sabotage
Typical timeline to failure:
	•	Week 1: Hero learns about new system (wasn't involved)
	•	Week 2: Hero tries it; finds problems
	•	Week 3: Hero voices concerns; feels ignored
	•	Week 4-6: Hero starts workarounds; tells colleagues it's broken
	•	Week 8+: System fails; hero blamed for "resistance to change"
Warning signs along the way:
	•	Implementation meetings without frontline workflow owners
	•	Requirements based on management's description, not staff reality
	•	Training scheduled as one-time event
	•	Hero not asked for input before launch
	•	No "champion" identified within the actual user group
REAL-WORLD EXAMPLE:
Scenario: A facility implemented AI-assisted billing without involving their veteran BOM—a 15-year employee who knew every payer quirk.
What they tried: IT and the vendor configured the system based on documentation. The BOM would be "trained" after go-live.
What went wrong:
	•	The BOM had developed 40+ payer-specific adjustments over 15 years—none documented
	•	AI system didn't account for her workarounds (which prevented denials)
	•	At training, she immediately identified problems
	•	She felt insulted: "Why didn't anyone ask me?"
	•	She told colleagues: "This doesn't work. Just do it the old way."
	•	Junior billers followed her lead
	•	Denial rate increased during first month due to AI recommendations
Consequences:
	•	BOM refused to use the system; created parallel process
	•	Vendor blamed facility; facility blamed vendor
	•	BOM considered retirement: "They don't value my experience"
	•	Pilot killed, but relationship damage was permanent
	•	Next AI initiative faced "will you actually listen to us this time?" skepticism
WHAT TO DO INSTEAD:
The correct approach: Hero-centered design. Identify your hero before implementation. Shadow them. Document their reality. Involve them in tool selection. Make them your champion.
Why this works better:
	•	Implementation matches reality because hero shaped it
	•	Hero's institutional knowledge is captured, not lost
	•	Hero feels valued and becomes champion
	•	When hero says "this works," everyone believes them
	•	Adoption happens because users see themselves in the solution
How to transition if already on wrong path: If you've excluded the hero:
	•	Stop and have an honest conversation with them
	•	Acknowledge you should have involved them earlier
	•	Ask for their input now—what's broken, what's missing?
	•	Make changes based on their feedback
	•	Publicly credit them for improvements
HOW CHATBOT SHOULD RESPOND:
When operator says: "We'll train them when it's ready" or "Staff are too busy to be involved"
What to say: "This is one of the most common—and most avoidable—failures I see.
Every building has a Workflow Hero. The scheduler who maintains 14 spreadsheets. The BOM who knows every payer quirk. The MDS nurse who catches errors at 9 PM.
These people hold institutional knowledge that doesn't exist anywhere else. They've built workarounds over years that prevent problems no one else knows about.
If you implement without them:
	•	You'll miss critical nuances
	•	The system won't match how work actually happens
	•	When they encounter it, they'll immediately see problems
	•	They'll feel excluded and unheard
	•	Their resistance will spread to the whole team
Hero-centered design. Find your hero. Shadow them. Document their reality. Involve them in tool selection. Make them your champion.
When the hero says 'this works,' everyone believes them.
Who is your hero for this workflow?"
Framework to apply: "Spreadsheet Hero" methodology, hero-centered design

ANTI-PATTERN 11: No Failure Insurance
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We're committed to making this work"
	•	"We've invested too much to fail"
	•	"We'll figure it out as we go"
	•	"Failure isn't an option"
Why it sounds reasonable at first: You don't want to be pessimistic. Planning for failure seems like you're setting yourself up to fail. Commitment and persistence are virtues. And who wants to talk about killing a project before it's even started?
Common phrases that indicate this mistake:
	•	"We're all-in on this"
	•	"We signed a multi-year contract"
	•	"We need to give it more time"
	•	"Killing it would be embarrassing"
WHY IT FAILS:
Root cause of failure: Without predefined kill criteria, you can't make rational decisions. When the pilot struggles (they all struggle), you don't know if it's normal friction or fatal flaws. Sunk cost fallacy kicks in. You keep investing in something that's not working because you can't admit it's not working.
What actually happens:
	•	Pilot launches with no defined failure criteria
	•	Early struggles emerge (normal for any pilot)
	•	Team debates: "Is this fixable or fundamental?"
	•	No objective criteria to resolve the debate
	•	Some want to kill; others want to persist
	•	Political dynamics take over; no rational decision
	•	Pilot limps along, wasting resources
	•	Eventually abandoned without clear lessons learned
Typical timeline to failure:
	•	Week 4: First significant issues
	•	Week 8: Debate begins—is this working?
	•	Week 12: "Extended pilot" to give it more time
	•	Week 20: Still debating
	•	Month 8: Quietly discontinued
	•	Month 9: No one willing to admit failure; no post-mortem
Warning signs along the way:
	•	No one can define "what would make us stop"
	•	Decisions based on feelings, not metrics
	•	"Just a little more time" becomes the mantra
	•	Sponsor avoids discussing whether to continue
	•	Resources drained from other priorities to keep it alive
REAL-WORLD EXAMPLE:
Scenario: A facility piloted AI scheduling with no predefined success or failure criteria. Just "try it and see."
What they tried: Launched the tool with goals of "improving efficiency" and "reducing agency spend."
What went wrong:
	•	At Week 4, results were mixed—some improvement in fill time, but agency spend unchanged
	•	Scheduler said she still preferred the old method for complex situations
	•	Administrator wanted to give it more time; HR said cut losses
	•	No objective criteria to resolve disagreement
	•	Pilot extended to "90 days" then "6 months"
	•	By Month 6, everyone had a different opinion but no data to prove anything
	•	Administrator left for a new role; successor quietly killed it
Consequences:
	•	6 months of subscription fees with unclear ROI
	•	Scheduler frustrated by being in limbo for half a year
	•	No clear lessons learned
	•	No formal post-mortem
	•	Next AI proposal met with "remember scheduling AI?"
WHAT TO DO INSTEAD:
The correct approach: Define kill criteria upfront. Before you start, agree: "We will kill this pilot if [X]." Make it objective. Write it down. Share it with stakeholders.
Kill criteria examples:
	•	Adds more than 1 step to the workflow
	•	Hero says "I hate it" after 4 weeks of honest effort
	•	Error rate increases by 10%
	•	Staff adoption drops below 50%
	•	No measurable time savings after 60 days
Why this works better:
	•	Decisions are objective, not political
	•	You can course-correct early if headed toward failure
	•	Killing becomes rational, not embarrassing
	•	Resources get freed for better opportunities
	•	Learning happens because you have clear criteria
How to transition if already on wrong path: If you're stuck in limbo without failure criteria:
	•	Call a stakeholder meeting immediately
	•	Define kill criteria now (better late than never)
	•	Agree on a 30-day decision deadline
	•	Measure against criteria honestly
	•	Make the call and move on
HOW CHATBOT SHOULD RESPOND:
When operator says: "Failure isn't an option" or "We're committed to making this work"
What to say: "I appreciate the commitment, but every experienced pilot manager will tell you: define your kill criteria before you start.
Without predefined failure criteria, you can't make rational decisions. When the pilot struggles—and all pilots struggle somewhere—you won't know if it's normal friction or a fatal flaw. Sunk cost fallacy kicks in. You keep investing in something broken because you can't admit it's broken.
Define kill criteria upfront:
Kill this pilot if:
	•	It adds steps to the workflow (even one)
	•	The hero hates it after 4 weeks of honest effort
	•	Error rate increases
	•	No measurable time savings after 60 days
Also define pivot criteria—when the workflow is right but the tool isn't.
And define scale criteria—clear wins that justify expansion.
This isn't pessimism. It's discipline. The best pilots have the clearest exit ramps.
What would make this pilot not worth continuing?"
Framework to apply: Failure Insurance (kill/pivot/scale criteria)

ANTI-PATTERN 12: Chasing AI Hype
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"ChatGPT is everywhere—we need to use it"
	•	"I read about AI agents that could run our whole operation"
	•	"Our competitors are doing AI—we can't fall behind"
	•	"The board saw a TED talk and now they want AI"
Why it sounds reasonable at first: AI is genuinely transformative. Media coverage is intense. Competitors claim AI adoption. Board members and executives are excited. You don't want to be the one who "missed the wave."
Common phrases that indicate this mistake:
	•	"We need to do something with AI"
	•	"What's everyone else doing?"
	•	"We saw [competitor] mentioned AI"
	•	"This is the future—we need to be part of it"
WHY IT FAILS:
Root cause of failure: Hype is not strategy. "Doing AI" is not a business case. When you chase headlines, you implement technology for its own sake rather than to solve specific problems. You end up with flashy demos that don't address real pain and expensive experiments that prove nothing.
What actually happens:
	•	Executive reads article / sees conference talk about AI
	•	Mandate comes down: "We should be doing AI"
	•	Team scrambles to find something—anything—to call an AI initiative
	•	Initiative chosen for impressiveness, not impact
	•	Implementation focuses on "we use AI" not "we solved X"
	•	Results are unclear because the goal was unclear
	•	Hype moves to the next thing; AI pilot forgotten
Typical timeline to failure:
	•	Month 1: Excitement from leadership about AI potential
	•	Month 2: Search for "what to do with AI"
	•	Month 3-4: Select initiative based on coolness factor
	•	Month 5-7: Implementation struggles (no clear problem being solved)
	•	Month 8: Leadership attention moves to next trend
	•	Year 1: AI initiative quietly abandoned
	•	Year 2: New hype cycle begins; repeat
Warning signs along the way:
	•	Business case centers on "AI" rather than outcomes
	•	Initiative chosen because it sounds impressive
	•	Success defined as "using AI" not "solving problem"
	•	Vendor selected because of AI credentials, not workflow fit
	•	Team can't articulate specific pain being addressed
REAL-WORLD EXAMPLE:
Scenario: A facility CEO heard about "AI agents" at a conference and returned excited. "We need AI agents working for us."
What they tried: Team scrambled to find an "AI agent" use case. They landed on an "AI concierge" that would answer family questions via text.
What went wrong:
	•	No one had identified family communication as a major pain point
	•	Actual pain points (prior auth, scheduling) weren't addressed
	•	AI concierge launched but barely used—families preferred calling
	•	Implementation was technically successful but operationally irrelevant
	•	CEO moved to new trend (blockchain, then IoT) within 6 months
	•	AI concierge quietly discontinued
Consequences:
	•	$60K spent on irrelevant initiative
	•	Staff cynical about leadership's "trend chasing"
	•	Real AI opportunities (scheduling, documentation) remained unaddressed
	•	Next legitimate AI proposal met with "Is this just another fad?"
WHAT TO DO INSTEAD:
The correct approach: Value-first, hype-agnostic. Start with the pain. Find the friction. Calculate the cost. Then ask: "Is AI the right solution for this problem?" Sometimes it is; sometimes a simpler automation works. Technology serves the problem, not the reverse.
Why this works better:
	•	You solve real problems, regardless of trend
	•	ROI is demonstrable because problem was real
	•	Staff see value, not theater
	•	When AI genuinely fits, it succeeds
	•	Your credibility survives the hype cycle
How to transition if already on wrong path: If you've been chasing hype:
	•	Pause and ask: "What problem are we actually solving?"
	•	If no clear answer, pivot to discovery process
	•	Identify real pain points with real metrics
	•	Only then ask "Is AI the right solution?"
	•	Reframe initiative around problem, not technology
HOW CHATBOT SHOULD RESPOND:
When operator says: "We need to do something with AI" or "Our competitors are using AI—we're falling behind"
What to say: "I hear this a lot, and I want to push back gently.
'Doing AI' is not a strategy. 'Using AI' is not a business case. When you chase headlines, you end up with flashy demos that don't solve real problems.
Here's the question I'd ask instead: What's your biggest operational pain point right now? What's costing you money, burning out your staff, or putting you at risk?
Start there. Find the friction. Calculate the cost. Then ask: 'Is AI the right solution?'
Sometimes AI genuinely is the answer. Sometimes a simple automation works better. Sometimes it's a process change, not technology at all.
Value-first, hype-agnostic. Solve the problem, not the headline.
What's actually keeping you up at night? Let's start there."
Framework to apply: Discovery process (problem-first)

ANTI-PATTERN 13: Generic AI Tools for Specific Problems
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We'll just use ChatGPT for documentation"
	•	"Why pay for a specialized tool when general AI works?"
	•	"Our staff can prompt Claude to do prior auths"
	•	"There's a free AI tool that can probably do this"
Why it sounds reasonable at first: General-purpose AI is incredibly capable. ChatGPT can write, analyze, summarize—almost anything. Why pay premium prices for specialized tools when the general tools are cheap or free?
Common phrases that indicate this mistake:
	•	"We can just prompt engineer our way to a solution"
	•	"General AI is good enough"
	•	"We don't need senior living-specific tools"
	•	"My admin assistant already uses ChatGPT for everything"
WHY IT FAILS:
Root cause of failure: General AI doesn't know your domain. It doesn't understand PDPM rules, Medicaid variations by state, or payer-specific prior auth requirements. It doesn't integrate with PCC. It doesn't know your workflows. Every use requires custom prompting, and results are inconsistent.
What actually happens:
	•	Staff try to use general AI for specialized tasks
	•	Initial results seem promising in demos
	•	Real use reveals gaps in domain knowledge
	•	Staff spend time fixing AI outputs instead of saving time
	•	Consistency degrades because each person prompts differently
	•	Quality varies wildly; some outputs are wrong
	•	Staff conclude "AI doesn't understand our work" and revert
Typical timeline to failure:
	•	Week 1-2: Exciting experiments with general AI
	•	Week 3-4: Staff discover gaps in domain knowledge
	•	Week 5-6: Time spent fixing outputs equals time "saved"
	•	Week 8+: Usage drops; too much friction
	•	Month 3: Nobody using it anymore
Warning signs along the way:
	•	Staff saying "It's good but I have to fix every output"
	•	Results vary dramatically between users
	•	AI makes errors that domain experts catch immediately
	•	No integration with actual workflow systems
	•	Staff treating AI as "cool experiment" not "real tool"
REAL-WORLD EXAMPLE:
Scenario: A facility's BOM started using ChatGPT to draft prior auth narratives instead of purchasing a specialized solution.
What they tried: BOM would paste clinical notes into ChatGPT with a prompt to "write a prior auth justification for skilled nursing."
What went wrong:
	•	ChatGPT didn't know payer-specific requirements
	•	It used outdated regulatory language
	•	It missed critical clinical details that experienced billers know to include
	•	Every output needed significant editing
	•	Time spent prompting + editing ≈ time to write from scratch
	•	BOM also concerned about HIPAA (pasting notes into public AI)
Consequences:
	•	BOM abandoned ChatGPT after 3 weeks
	•	Told colleagues: "AI doesn't understand Medicaid"
	•	Real specialized solution was now harder to sell internally
	•	Facility concluded AI wasn't ready for billing (when actually they just used wrong tool)
WHAT TO DO INSTEAD:
The correct approach: Domain-specific solutions for domain-specific problems. For senior living workflows, use tools built for senior living. They know PDPM. They understand payer variations. They integrate with EHRs. They've been trained on your exact use cases.
Why this works better:
	•	Domain knowledge is built in, not prompted
	•	Integration with your actual systems
	•	Consistent outputs because the tool is configured for your use case
	•	Compliance considerations addressed (HIPAA, data handling)
	•	Support team understands your industry
How to transition if already on wrong path: If you've been using general AI with poor results:
	•	Document what didn't work (helps you evaluate specialized tools)
	•	Research senior living-specific solutions for your use case
	•	Ask vendors: "What do you know about SNF/ALF workflows that ChatGPT doesn't?"
	•	Pilot a specialized tool against your documented requirements
	•	Compare results (don't just assume specialized is better—prove it)
HOW CHATBOT SHOULD RESPOND:
When operator says: "Can't we just use ChatGPT for this?" or "Why pay for specialized tools?"
What to say: "General AI is impressively capable, but here's the catch: it doesn't know your domain.
ChatGPT doesn't understand PDPM rules. It doesn't know payer-specific prior auth requirements. It doesn't know the difference between Idaho Medicaid and California Medicaid. It doesn't integrate with PCC.
When you use general AI for specialized tasks:
	•	Every output needs fixing
	•	Results vary by who's prompting
	•	You miss domain nuances experts catch immediately
	•	Time spent prompting + editing often equals time to do it manually
	•	HIPAA questions arise (should you paste clinical notes into public AI?)
For senior living workflows—prior auth, documentation, billing, compliance—use tools built for senior living. They know your rules. They integrate with your systems. They've been trained on your exact use cases.
General AI is great for general tasks. Specialized problems need specialized solutions.
What specific workflow are you trying to solve? Let's evaluate the right tool type."
Framework to apply: Tool selection criteria, domain-specific vs. general AI

ANTI-PATTERN 14: Skipping Change Management
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We'll just roll it out and people will adapt"
	•	"Our team is used to new systems"
	•	"The tool is intuitive—they won't need much training"
	•	"Change management is for big projects; this is just a pilot"
Why it sounds reasonable at first: Your staff are professionals who adapt to new systems regularly. New EHR modules, new compliance requirements, new documentation templates—they've handled it all. A single AI tool seems small by comparison. Over-engineering the rollout seems like overkill.
Common phrases that indicate this mistake:
	•	"It's user-friendly—they'll figure it out"
	•	"We don't have time for elaborate change management"
	•	"Quick email announcement, then go live"
	•	"If they don't use it, that's on them"
WHY IT FAILS:
Root cause of failure: AI triggers unique fears that other technology doesn't. "Will this replace me?" "Is the computer judging my work?" "What if I look stupid using it?" Without addressing these fears, staff will resist—not because the tool is bad, but because they're scared.
What actually happens:
	•	Tool launches with minimal communication
	•	Staff hear about it through gossip before official announcement
	•	Fears spread: "They're trying to replace us"
	•	Staff approach the tool with anxiety and skepticism
	•	First struggles confirm fears: "See, it doesn't work"
	•	Resistance becomes cultural norm
	•	Tool fails not because of functionality but because of psychology
Typical timeline to failure:
	•	Week -1: Rumors start before official announcement
	•	Week 0: Surprise launch; staff feel blindsided
	•	Week 1: Anxious adoption; looking for problems
	•	Week 2-3: Early struggles amplified by fear
	•	Week 4+: Cultural resistance solidifies
	•	Month 2+: Tool abandoned; blamed on "resistant staff"
Warning signs along the way:
	•	Staff asking "Why didn't anyone tell us?"
	•	Water cooler talk about job replacement
	•	Staff looking for reasons the tool doesn't work
	•	Best users staying quiet (afraid to be seen as "pro-AI")
	•	Compliance through minimal engagement, not genuine adoption
REAL-WORLD EXAMPLE:
Scenario: A facility deployed AI documentation assistance with a one-email announcement. "Starting Monday, you'll have access to AI note assistance. See attached quick-start guide."
What they tried: Quick rollout to minimize disruption to operations.
What went wrong:
	•	Staff heard through rumors before the email
	•	Email didn't address the unspoken question: "Is this to replace us?"
	•	Nurses assumed management was tracking who used AI (surveillance fear)
	•	Staff who liked it stayed quiet (didn't want to seem pro-automation)
	•	Staff who hated it were vocal
	•	Cultural narrative: "They're trying to replace us with computers"
	•	Adoption was minimal despite functional tool
Consequences:
	•	Tool adoption under 20% after 60 days
	•	Nursing staff morale dropped
	•	Two nurses cited AI as factor in resignation
	•	Management blamed "change-resistant staff"
	•	Real lesson (change management matters) never learned
WHAT TO DO INSTEAD:
The correct approach: Communication from Day 1. Address fears before they fester. Explain the "why." Emphasize load reduction, not replacement. Identify champions. Create psychological safety.
Change management essentials:
	•	Announce intention 2-3 weeks before launch
	•	Explain WHY (reduce burden, not replace people)
	•	Address fears directly ("Your job is not at risk")
	•	Identify and celebrate early adopters
	•	Create safe space for feedback
	•	Make it okay to say "I'm struggling with this"
Why this works better:
	•	Fears addressed before they spread
	•	Staff feel respected, not surprised
	•	Champions emerge to help skeptics
	•	Feedback improves the implementation
	•	Adoption becomes cultural, not mandated
How to transition if already on wrong path: If you've launched without change management:
	•	Acknowledge the gap publicly: "We should have communicated better"
	•	Hold an all-hands to address fears directly
	•	Invite feedback—and act on it
	•	Identify and empower champions
	•	Reframe as "we're learning together" not "you must adopt"
HOW CHATBOT SHOULD RESPOND:
When operator says: "We'll just roll it out and people will adapt" or "It's intuitive—they won't need training"
What to say: "AI triggers unique fears that other technology doesn't. 'Will this replace me?' 'Is the computer judging my work?' 'What if I look stupid?'
Without addressing these fears, staff will resist—not because the tool is bad, but because they're scared.
Change management essentials:
	•	Announce 2-3 weeks before launch. No surprises.
	•	Explain the 'why.' 'We're reducing your paperwork burden, not replacing you.'
	•	Address fears directly. 'Your job is not at risk.'
	•	Find champions. The nurses who try it first and say 'This actually helps.'
	•	Create safety. It's okay to struggle. It's okay to ask for help.
	•	Celebrate wins publicly. 'Sarah saved 30 minutes today using the new tool.'
When you skip this, you get rumors, fear, resistance, and failure—even when the tool is great.
Who's going to be your champion? And how will you address the fear of job replacement?"
Framework to apply: Change management methodology, "Load reduction not job replacement" messaging

ANTI-PATTERN 15: ROI Calculated on Assumptions
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"This should save about 2 hours per day"
	•	"The vendor says we'll reduce costs by 30%"
	•	"If this saves even 10 nurses 30 minutes each, that's 5 hours a day"
	•	"We'll track ROI after we see how it works"
Why it sounds reasonable at first: You need to project ROI to get budget approved. Exact numbers are impossible before implementation. Reasonable estimates seem better than nothing. And the vendor has data from other facilities, so their projections must be grounded.
Common phrases that indicate this mistake:
	•	"Based on industry benchmarks..."
	•	"The vendor projects..."
	•	"We estimate approximately..."
	•	"Our assumptions are conservative"
WHY IT FAILS:
Root cause of failure: Assumed ROI can't be defended. When CFO asks "Did this deliver?" you have no baseline to compare. When vendor projections don't materialize, there's no accountability. You end up with vague claims of improvement that don't satisfy financial scrutiny.
What actually happens:
	•	Business case built on assumptions ("save 2 hours per person")
	•	No baseline measured before implementation
	•	Pilot runs with assumed benefits in mind
	•	At pilot end, no data to prove or disprove assumptions
	•	CFO asks: "You said 2 hours per person. Did we get that?"
	•	Answer: "We think so, but we didn't measure before..."
	•	CFO can't justify continued investment without proof
	•	Project dies despite possibly being successful
Typical timeline to failure:
	•	Month 0: Assumptions drive business case
	•	Month 1-3: Pilot runs
	•	Month 4: ROI review—no baseline data
	•	Month 4+: "Extended pilot" to gather data retroactively
	•	Month 6+: Never-ending debate about whether it worked
Warning signs along the way:
	•	No time studies before implementation
	•	ROI projections based on vendor claims
	•	"We'll measure ROI later" mentioned in planning
	•	Can't answer "How long does this take today?"
	•	Success defined in percentages without absolute baselines
REAL-WORLD EXAMPLE:
Scenario: A facility built a business case for AI documentation: "If we save nurses 45 minutes per shift, that's $X in value." CFO approved.
What they tried: Launched AI documentation with projected savings of 45 minutes per nurse per shift based on vendor case studies.
What went wrong:
	•	No one measured how long documentation actually took before the pilot
	•	"45 minutes per shift" was vendor's average across all clients
	•	At pilot end, nurses said "It helps" but couldn't quantify how much
	•	CFO asked: "Did we hit 45 minutes?" No data to answer.
	•	Some nurses spent the "saved time" doing other documentation, so shift length unchanged
	•	CFO couldn't verify any savings; declined to renew
Consequences:
	•	Possibly-effective tool discontinued due to unmeasured ROI
	•	Vendor blamed facility; facility blamed vendor
	•	Future AI proposals required "guaranteed ROI"—impossible to provide
	•	Real lesson: Measure baseline before starting
WHAT TO DO INSTEAD:
The correct approach: Measure baseline first. Before any pilot, spend one week measuring the current state. Then track the same metrics during and after implementation. Let the data tell the story.
Baseline measurement essentials:
	•	Time studies (how long does this take today?)
	•	Touch counts (how many steps/clicks/handoffs?)
	•	Error rates (how often does this go wrong?)
	•	Volume (how many per day/week?)
Why this works better:
	•	You can definitively answer "Did this work?"
	•	CFO has before/after data, not assumptions
	•	If you're successful, the win is undeniable
	•	If you're not, you know sooner and can adjust
	•	Vendor claims can be validated against your reality
How to transition if already on wrong path: If you're mid-pilot without baseline:
	•	Measure current state NOW (better late than never)
	•	Track remaining pilot period against this new baseline
	•	Acknowledge in reporting that baseline is partial
	•	For next pilot, make baseline measurement Week -1
HOW CHATBOT SHOULD RESPOND:
When operator says: "The vendor says we'll save 30%" or "We estimate approximately..."
What to say: "Projections are nice, but they can't be defended.
Here's what typically happens: You build a business case on assumptions. Pilot runs. At the end, CFO asks 'Did we hit the projections?' And you can't answer because you never measured the baseline.
Measure first. Assume never.
Before your pilot starts, spend one week measuring current state:
	•	How long does this task actually take today? (Time 10 instances, average them)
	•	How many steps/clicks/handoffs?
	•	What's the error rate?
Then track the same metrics during the pilot. At the end, you have before/after data—not assumptions.
This lets you say: 'Prior auth narratives took 47 minutes on average. With AI, they take 22 minutes. That's a 53% reduction, saving 25 minutes per auth.'
That's a story a CFO can believe.
Do you know your current baseline for this workflow?"
Framework to apply: Baseline measurement methodology, metrics framework

ANTI-PATTERN 16: Ignoring Vendor Sustainability
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"This startup has the best technology"
	•	"They're the most innovative vendor"
	•	"Yes they're new, but their demo was amazing"
	•	"The bigger vendors are too slow/expensive"
Why it sounds reasonable at first: Innovation often comes from startups. Smaller vendors can be more agile, more affordable, and more attentive. The "best" technology might genuinely be from a company you've never heard of.
Common phrases that indicate this mistake:
	•	"They're venture-backed so they have resources"
	•	"They move faster than the big guys"
	•	"We'll be one of their first customers"
	•	"They're growing fast"
WHY IT FAILS:
Root cause of failure: Healthcare software requires long-term commitment. AI vendors in healthcare need to invest in compliance, security, integrations, and ongoing maintenance. Many startups don't survive their first 2-3 years. If your vendor fails, your system dies with it.
What actually happens:
	•	Select innovative startup with great demo
	•	Implementation goes well initially
	•	Year 2: Vendor struggles to raise additional funding
	•	Support quality degrades as staff leave
	•	Updates slow or stop
	•	Year 3: Vendor acquired, product discontinued, or company fails
	•	You're stuck: migrate to new system or maintain unsupported tool
Typical timeline to failure:
	•	Year 1: Great relationship, innovative features
	•	Year 1.5: Support tickets take longer to resolve
	•	Year 2: Key contacts leave vendor
	•	Year 2.5: Vendor announces "strategic pivot" or "acquisition"
	•	Year 3: Product sunsetted; you're on your own
Warning signs along the way:
	•	Vendor's client list is all recent (last 12 months)
	•	High turnover in your account team
	•	Feature roadmap keeps slipping
	•	Vendor avoids questions about funding/runway
	•	No healthcare-specific clients similar to your size
REAL-WORLD EXAMPLE:
Scenario: A facility chose an AI startup for documentation assistance. Best demo, best price, most innovative features.
What they tried: 2-year contract with innovative AI startup that had raised Series A funding.
What went wrong:
	•	Year 1 went well; features delivered as promised
	•	Year 2: Vendor struggled to raise Series B
	•	Support response times went from hours to weeks
	•	Key engineers left; bug fixes stopped
	•	Vendor announced acquisition by larger company
	•	Acquirer discontinued the specific product line
	•	Facility given 90 days to migrate
Consequences:
	•	$80K in sunk implementation costs
	•	4 months of emergency migration project
	•	Staff retrained on completely new system
	•	Lost 18 months of workflow refinement
	•	Trust in "innovative" vendors permanently damaged
WHAT TO DO INSTEAD:
The correct approach: Evaluate sustainability alongside capability. Ask hard questions about funding, runway, client base, and long-term plans. Balance innovation against stability. Consider what happens if this vendor disappears.
Sustainability questions to ask:
	•	How long have you been in business?
	•	How many healthcare clients do you have?
	•	What's your funding situation? (At least 24 months runway)
	•	Who are your senior living references?
	•	What happens to my data if you're acquired or shut down?
	•	Can I export my configuration if needed?
Why this works better:
	•	You avoid being stranded when a vendor fails
	•	Long-term relationship enables continuous improvement
	•	Stable vendors invest in compliance and security
	•	You can plan for years, not just months
How to transition if already on wrong path: If you're with a vendor you're worried about:
	•	Review contract terms for exit/migration provisions
	•	Ensure data can be exported
	•	Document your configurations and customizations
	•	Identify backup vendors before you need them
	•	Build contingency timeline for migration
HOW CHATBOT SHOULD RESPOND:
When operator says: "This startup has the best demo" or "They're the most innovative vendor"
What to say: "Innovation is great, but sustainability matters in healthcare.
AI vendors need to invest in compliance, security, integrations, and ongoing support. Many startups don't survive their first 2-3 years. If your vendor fails, your system dies with it.
Ask these sustainability questions:
	•	How long have you been in business?
	•	How many healthcare/senior living clients do you have?
	•	What's your funding situation? Do you have at least 24 months runway?
	•	What happens to my data if you're acquired or shut down?
	•	Can I export my configuration if I need to migrate?
Best demo doesn't mean best long-term partner.
This doesn't mean avoid startups entirely—just go in with eyes open. For critical workflows, stability might matter more than bleeding-edge features.
What's this vendor's track record in healthcare?"
Framework to apply: Vendor evaluation (sustainability criteria)

ANTI-PATTERN 17: Treating AI as "Set and Forget"
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"Once it's set up, it runs itself"
	•	"AI should just work"
	•	"We don't have bandwidth for ongoing management"
	•	"The vendor handles everything after launch"
Why it sounds reasonable at first: You're implementing AI to reduce work, not add it. Ongoing management sounds like more burden. And the vendor's job is to make it work, right?
Common phrases that indicate this mistake:
	•	"We'll set it up and let it run"
	•	"AI is supposed to be automated"
	•	"We need a low-maintenance solution"
	•	"After launch we move to other priorities"
WHY IT FAILS:
Root cause of failure: AI systems drift. Payer rules change. Staff turnover means new users need training. Edge cases emerge that weren't anticipated. Without ongoing attention, AI accuracy degrades, workarounds emerge, and the tool becomes less valuable over time.
What actually happens:
	•	AI launched, seems to work well
	•	Team moves to other priorities
	•	Month 3: Small errors start appearing
	•	Nobody notices because nobody's monitoring
	•	Month 6: Workarounds become standard practice
	•	Year 1: AI tool is running but barely used/trusted
	•	Renewal time: "Why are we paying for this?"
Typical timeline to failure:
	•	Month 1-3: Post-launch, working well
	•	Month 4-6: Drift begins, unnoticed
	•	Month 7-12: Usage patterns degrade
	•	Year 2: Tool renewed on autopilot despite low value
	•	Year 3: Tool discontinued as cost-cutting measure
Warning signs along the way:
	•	No regular review of AI outputs
	•	Staff creating workarounds silently
	•	No one can report on AI performance metrics
	•	New staff never trained on AI tools
	•	Vendor relationship is billing-only
REAL-WORLD EXAMPLE:
Scenario: A facility implemented AI billing assistance. It worked great at launch. Team moved on to other priorities.
What they tried: "Set and forget"—no ongoing monitoring or management.
What went wrong:
	•	Payer requirements changed in Q2; AI wasn't updated
	•	New biller started in Month 4; minimal AI training
	•	By Month 6, only one biller used AI regularly
	•	Errors crept in due to outdated rules
	•	By Year 1, AI tool was running but mostly ignored
	•	At renewal, CFO asked: "What value are we getting?" No one could answer.
Consequences:
	•	50% of potential value unrealized
	•	Renewal decision based on inertia, not performance
	•	Year 2 cost justified by Year 1 assumptions (never validated)
	•	Eventually discontinued without understanding why it degraded
WHAT TO DO INSTEAD:
The correct approach: Ongoing stewardship. Assign an owner. Schedule monthly reviews. Monitor accuracy. Retrain new staff. Update for rule changes. Treat AI as a living system, not a static installation.
Ongoing management essentials:
	•	Monthly accuracy review (sample 10-20 outputs)
	•	New staff onboarding includes AI tool training
	•	Quarterly rules/process update review
	•	Annual performance assessment
	•	Named owner responsible for AI health
Why this works better:
	•	Issues caught early before they compound
	•	New staff contribute to adoption, not erosion
	•	AI stays aligned with changing reality
	•	Value is demonstrated continuously
	•	Renewal decisions are informed by data
How to transition if already on wrong path: If your AI has been running unmanaged:
	•	Assign an owner (even part-time)
	•	Conduct an immediate accuracy audit
	•	Survey users on current satisfaction
	•	Update any outdated configurations
	•	Establish monthly review cadence going forward
HOW CHATBOT SHOULD RESPOND:
When operator says: "Once it's set up, it runs itself" or "AI should be automated"
What to say: "AI reduces work but isn't work-free. Here's what I've seen:
AI systems drift. Payer rules change. Staff turns over. Edge cases emerge. Without ongoing attention, accuracy degrades, workarounds appear, and the tool loses value.
Ongoing stewardship essentials:
	•	Monthly accuracy review: Sample 10-20 AI outputs. Are they still good?
	•	New staff training: Every new hire learns the AI tools
	•	Quarterly updates: Rules, processes, configurations—are they current?
	•	Named owner: Someone responsible for AI tool health
	•	Annual assessment: Performance review with usage data
This doesn't have to be heavy—2-4 hours per month for most pilots. But zero attention guarantees gradual failure.
Who's going to be the steward for this AI tool after launch?"
Framework to apply: Ongoing stewardship model

ANTI-PATTERN 18: Confusing Pilot Success with Organizational Readiness
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"The pilot worked in Building A—let's roll it out everywhere"
	•	"We proved the concept—time to scale"
	•	"Leadership is excited; we're going organization-wide"
	•	"We don't have time for phased rollout"
Why it sounds reasonable at first: You did the pilot. It worked. Leadership is supportive. Momentum is high. Why slow down with phased approaches when you've already proven it works?
Common phrases that indicate this mistake:
	•	"We already proved it—just roll it out"
	•	"All buildings should get this"
	•	"Let's capture value across the organization"
	•	"Phased rollout is too slow"
WHY IT FAILS:
Root cause of failure: Pilot success doesn't transfer automatically. Your pilot building had specific conditions: engaged administrator, stable staff, workflow champion, extra attention. Other buildings don't have those conditions. What worked there won't work automatically everywhere.
What actually happens:
	•	Pilot succeeds in carefully selected building
	•	Organization-wide rollout announced
	•	Other buildings don't have the same conditions
	•	Same tool performs poorly in different environments
	•	Staff in failing buildings blame the tool
	•	Success of original pilot gets overshadowed by scale problems
	•	Organization-wide adoption fails despite pilot success
Typical timeline to failure:
	•	Month 0: Pilot success celebrated
	•	Month 1: Organization-wide rollout begins
	•	Month 2: Problems emerge at 50% of new buildings
	•	Month 3: Resources scrambled across multiple sites
	•	Month 4: Some buildings succeed, many struggle
	•	Month 6: Overall adoption below expectations
	•	Month 12: Tool used inconsistently; "mixed results"
Warning signs along the way:
	•	Rollout plan doesn't include site readiness assessment
	•	No champion identified at each new building
	•	Training is one-size-fits-all
	•	Buildings with different workflows treated identically
	•	Original pilot building conditions not documented
REAL-WORLD EXAMPLE:
Scenario: A pilot of AI scheduling assistance succeeded beautifully at a suburban SNF with an experienced, stable scheduling team and engaged administrator.
What they tried: Rolled out immediately to all 15 facilities—urban, rural, SNF, ALF, various staffing models.
What went wrong:
	•	Urban buildings had union considerations pilot didn't address
	•	Rural buildings had different agency relationships
	•	ALFs had simpler scheduling needs—tool was overkill
	•	Three buildings had scheduler turnover during rollout
	•	Training was designed for pilot building's workflow
	•	60% of buildings struggled; 40% succeeded
Consequences:
	•	Instead of 15 successes, had ~6 successes and ~9 struggles
	•	Narrative shifted from "great AI win" to "mixed results"
	•	Resources consumed by struggling buildings
	•	Successful pilot nearly forgotten in noise of scale problems
	•	Organization-wide adoption took 18 months instead of 3
WHAT TO DO INSTEAD:
The correct approach: Phased expansion with site readiness. After pilot success, add 2-3 buildings at a time. Assess readiness at each site. Identify local champions. Adapt training to local workflows. Build capability before scaling broadly.
Expansion methodology:
	•	Wave 1: Original pilot building
	•	Wave 2: 2-3 buildings with strongest readiness
	•	Wave 3: Next 3-4 buildings with medium readiness
	•	Wave 4+: Remaining buildings with appropriate support
Why this works better:
	•	Each wave builds capability for the next
	•	Struggling buildings get attention without overwhelming resources
	•	Success compounds instead of diluting
	•	Local champions develop and can help other buildings
	•	Lessons learned improve each subsequent wave
How to transition if already on wrong path: If you've already tried to scale too fast:
	•	Pause rollout at struggling buildings
	•	Focus resources on getting a few more to success
	•	Document what made successful buildings work
	•	Assess remaining buildings for readiness
	•	Resume rollout in waves, not all at once
HOW CHATBOT SHOULD RESPOND:
When operator says: "The pilot worked—let's roll it out everywhere"
What to say: "I'm excited about your pilot success! But here's a pattern I've seen derail great results:
Your pilot building had specific conditions: engaged administrator, stable staff, champion, extra attention. Other buildings don't automatically have those conditions.
Phased expansion works better:
	•	Wave 1: Your pilot building (done!)
	•	Wave 2: Add 2-3 buildings with strongest readiness
	•	Wave 3: Next 3-4 buildings with lessons learned
	•	Continue until organization-wide
Before adding each building, assess:
	•	Does the administrator support this?
	•	Who's the local champion?
	•	Is the workflow similar enough to the pilot?
	•	Is staffing stable enough to adopt something new?
This takes longer but produces 15 successes instead of 6 successes and 9 struggles.
Which buildings are most similar to your pilot? Let's start there."
Framework to apply: Phased expansion methodology, site readiness assessment

ANTI-PATTERN 19: Over-Relying on AI Accuracy
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"The AI is 95% accurate—we can trust it"
	•	"We'll just use whatever AI recommends"
	•	"AI makes fewer errors than humans"
	•	"We can reduce QA since AI is so accurate"
Why it sounds reasonable at first: AI accuracy statistics are impressive. If AI is right 95% of the time, that sounds better than human error rates. Less QA means more efficiency. Trust in AI is what makes it valuable.
Common phrases that indicate this mistake:
	•	"AI is more consistent than our staff"
	•	"We don't need to check every output"
	•	"The algorithm has been validated"
	•	"We can process faster by trusting AI"
WHY IT FAILS:
Root cause of failure: In healthcare, 5% errors can be catastrophic. 95% accuracy means 1 in 20 is wrong. If you process 100 claims per week, that's 5 errors weekly. And AI errors aren't random—they cluster around edge cases and unusual situations, exactly where you most need accuracy.
What actually happens:
	•	AI deployed with reduced human oversight
	•	Most outputs are fine (95%)
	•	Errors slip through in edge cases
	•	Errors compound—wrong codes lead to wrong payments
	•	Problems discovered weeks or months later
	•	Retrospective fixes are expensive (clawbacks, resubmissions)
	•	Trust collapses when error pattern is revealed
Typical timeline to failure:
	•	Month 1: AI running, outputs look good
	•	Month 2: First errors discovered; dismissed as outliers
	•	Month 3-4: Error pattern emerges
	•	Month 5: Major issue (audit finding, significant denial, compliance question)
	•	Month 6: QA reinstated; AI value proposition questioned
Warning signs along the way:
	•	QA bypassed because "AI is accurate"
	•	No sampling process for AI outputs
	•	Errors attributed to other causes before AI checked
	•	Over-confidence in accuracy statistics
	•	AI used for high-risk decisions without verification
REAL-WORLD EXAMPLE:
Scenario: A billing team reduced QA processes after implementing AI coding assistance. "AI is 93% accurate—better than our average coder."
What they tried: Processed claims without human verification, trusting AI recommendations.
What went wrong:
	•	AI struggled with complex co-morbidities (edge cases)
	•	7% error rate concentrated in highest-acuity patients
	•	Those patients had the most complex (and valuable) claims
	•	Payer audit 6 months later found systematic coding errors
	•	$180K in clawbacks plus corrective action plan
Consequences:
	•	Clawbacks eliminated ROI from AI implementation
	•	Compliance audit triggered ongoing monitoring
	•	QA fully reinstated plus AI verification layer
	•	Biller who championed reduced QA disciplined
	•	AI blamed for human decision to reduce oversight
WHAT TO DO INSTEAD:
The correct approach: AI augments, never replaces, human judgment. Even with high accuracy AI, maintain oversight processes. Sample outputs regularly. Build feedback loops. Be especially vigilant with edge cases and high-value decisions.
Oversight model:
	•	High-risk decisions: 100% human review
	•	Medium-risk: Statistical sampling (20-30%)
	•	Low-risk: Periodic sampling (5-10%)
	•	All: Feedback mechanism when errors found
Why this works better:
	•	Errors caught before they compound
	•	AI improves over time with feedback
	•	Compliance posture protected
	•	Staff stay skilled (can catch AI errors)
	•	Trust in AI remains appropriate
How to transition if already on wrong path: If you've reduced oversight too much:
	•	Immediately reinstate sampling/QA
	•	Conduct retrospective review of AI outputs
	•	Identify error patterns
	•	Implement appropriate oversight by risk level
	•	Train staff that "AI assists, humans decide"
HOW CHATBOT SHOULD RESPOND:
When operator says: "AI is 95% accurate—we can reduce QA" or "We can trust AI recommendations"
What to say: "95% accuracy sounds great until you do the math for healthcare.
95% means 1 in 20 is wrong. If you process 100 claims per week, that's 5 errors weekly. In healthcare, those errors can mean:
	•	Compliance issues
	•	Clawbacks
	•	Survey findings
	•	Patient care problems
And AI errors aren't random—they cluster in edge cases, which are often your highest-acuity, highest-value situations.
AI augments, never replaces, human judgment.
Even with high-accuracy AI:
	•	High-risk decisions: 100% human review
	•	Medium-risk: Sample 20-30%
	•	Low-risk: Sample 5-10%
	•	Always: Feedback mechanism when you catch errors
The goal is AI makes humans faster and more consistent, not AI makes humans unnecessary.
How are you planning to maintain oversight?"
Framework to apply: Human-in-the-loop principles, risk-based oversight

ANTI-PATTERN 20: Building Before Buying
WHAT IT LOOKS LIKE:
How operators typically describe this approach:
	•	"We'll build our own AI solution"
	•	"Our IT team can create this internally"
	•	"We have developers who can build exactly what we need"
	•	"Why pay a vendor when we can build it ourselves?"
Why it sounds reasonable at first: You know your needs best. Custom solutions can fit perfectly. No vendor markup. No dependency on external companies. Your IT team wants the project. Control over your own destiny.
Common phrases that indicate this mistake:
	•	"We can do this cheaper internally"
	•	"We want full control"
	•	"Our developers are skilled with AI"
	•	"Vendors don't understand our specific needs"
WHY IT FAILS:
Root cause of failure: Building AI in healthcare is massively complex. Beyond the AI model itself, you need compliance infrastructure, security protocols, ongoing training data, edge case handling, documentation, and maintenance. What looks like a 3-month project becomes an 18-month odyssey.
What actually happens:
	•	IT estimates 3-6 months to build
	•	Prototype works in demo but not in production
	•	Edge cases multiply; complexity explodes
	•	Compliance requirements add 6 months
	•	Original developer leaves; knowledge lost
	•	Maintenance burden consumes IT resources
	•	18-24 months later: inferior solution to what vendors offer
Typical timeline to failure:
	•	Month 1-3: Prototype development
	•	Month 4-6: Integration challenges
	•	Month 7-12: Edge cases, compliance, security
	•	Month 12-18: "Almost done" phase that never ends
	•	Month 18+: Either abandoned or limps along
	•	Year 2: Cost exceeds vendor pricing many times over
Warning signs along the way:
	•	Scope creeping beyond original estimate
	•	Key developer becomes irreplaceable
	•	Documentation falls behind
	•	Edge cases consume development time
	•	Security review keeps finding issues
	•	"Almost done" becomes recurring status
REAL-WORLD EXAMPLE:
Scenario: An operator decided to build their own AI documentation assistant. "Our IT team knows Python and we can build exactly what we need."
What they tried: Internal development project estimated at 4 months.
What went wrong:
	•	Prototype worked on simple notes; failed on complex ones
	•	HIPAA compliance review added 5 months of requirements
	•	Integration with PCC was far more complex than expected
	•	Lead developer left at month 10; knowledge largely undocumented
	•	At month 18, tool still unreliable; staff didn't trust it
	•	Total cost: $400K+ (dev time, security, lost opportunity)
	•	Commercial alternative: $60K/year
Consequences:
	•	18 months with no working solution
	•	$400K invested with poor results
	•	IT team burned out and frustrated
	•	Commercial vendor eventually purchased anyway
	•	Total cost: $460K for what could have been $60K
WHAT TO DO INSTEAD:
The correct approach: Buy before build. Evaluate commercial solutions first. Only build if no vendor can address your needs AND you have realistic resources AND you're committed for the long term.
Decision criteria:
	•	Does a commercial solution exist? → Try it first
	•	Is customization truly impossible with vendors? → Probably not
	•	Do you have dedicated resources for years (not months)? → Usually no
	•	Is this your core business? → For healthcare operators, no
Why this works better:
	•	Vendors have solved compliance, security, edge cases
	•	Time to value is months, not years
	•	Risk is shared with vendor
	•	You can switch vendors; you can't abandon custom builds
	•	Your IT focuses on core operations
How to transition if already on wrong path: If you've started building internally:
	•	Honest assessment: How far behind schedule? How much more to do?
	•	Calculate true total cost (including opportunity cost)
	•	Evaluate commercial alternatives NOW
	•	Make hard decision: continue or pivot
	•	If pivoting, salvage what you can (requirements, data) for vendor selection
HOW CHATBOT SHOULD RESPOND:
When operator says: "We'll build our own AI" or "Our IT can create this"
What to say: "I'd strongly encourage you to buy before you build.
Here's what typically happens with internal AI builds:
	•	3-6 month estimate becomes 18-24 months
	•	Prototype works; production version fails on edge cases
	•	Compliance, security, integration add massive complexity
	•	Key developers leave; knowledge lost
	•	Total cost exceeds vendor pricing by 5-10x
AI in healthcare is massively complex beyond the model itself. Vendors have solved compliance, security, and edge cases through years of investment. You'd be rebuilding all of that.
Buy before build:
	•	Does a commercial solution exist? → Try it first
	•	Can vendors customize for your needs? → Usually yes
	•	Do you have dedicated resources for years? → Be honest
	•	Is AI development your core business? → No, you're a healthcare operator
Your IT team's time is better spent on your core operations. Let AI vendors do what they specialize in.
What specifically makes you think you need to build custom?"
Framework to apply: Buy vs. build decision criteria

SUMMARY: ANTI-PATTERN QUICK REFERENCE
#
Anti-Pattern
Failure Mode
Correct Approach
1
Starting with Clinical
High stakes, low tolerance
Start with admin burden
2
IT-Led Initiative
Workflow blindness
Ops-led, IT-enabled
3
Multi-Building Pilots
Complexity explosion
One building at a time
4
Vendor-First
Solution seeking problem
Problem-first, then vendor
5
Skip Workflow Mapping
Implementation misfit
Shadow, document, then build
6
No Success Metrics
Can't prove value
Define metrics Week 1
7
Over-Customization
Cost/complexity spiral
80% solution first
8
Multiple Simultaneous Pilots
Divided attention
Sequential pilots
9
Transform Everything
Paralysis
One win, then scale
10
Ignore Workflow Hero
No adoption
Hero-centered design
11
No Failure Insurance
Sunk cost trap
Kill criteria upfront
12
Chase AI Hype
Trendy ≠ valuable
Value-first
13
Generic AI for Specific
Domain mismatch
Domain-specific tools
14
Skip Change Management
Fear/resistance
Communication from Day 1
15
ROI on Assumptions
Can't defend claims
Measure baseline first
16
Ignore Vendor Sustainability
Stranded by failure
Evaluate long-term
17
Set and Forget
Gradual degradation
Ongoing stewardship
18
Pilot ≠ Organizational Ready
Scale problems
Phased expansion
19
Over-Rely on AI Accuracy
Undetected errors
Human-in-the-loop
20
Build Before Buy
Cost/time explosion
Buy first, build never

This anti-pattern library serves as the chatbot's "protective knowledge"—enabling it to catch operators before they make expensive, time-consuming mistakes and redirect them to approaches that actually work.

