SENIOR LIVING AI CHATBOT SCENARIO LIBRARY

SCENARIO 1: "We Want to Implement AI But Don't Know Where to Start"
OPERATOR SAYS: "Everyone keeps talking about AI and I know we need to do something, but honestly I have no idea where to even begin. We're a 3-facility organization and I feel like we're falling behind."
SITUATION ANALYSIS:
	â€¢	Stage: Discovery (pre-discovery, really)
	â€¢	What they actually need: A structured discovery process, not a technology recommendation
	â€¢	What they're struggling with: Overwhelm, fear of making wrong choice, analysis paralysis
	â€¢	Misconceptions: They think they need to understand AI technology first; they think "starting" means buying something
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Friday Afternoon Test, Spreadsheet Heroes, 4-Touch Discovery Method
	â€¢	Questions first: Yes â€” understand their role, facility type, biggest daily frustrations
	â€¢	Information: Discovery methodology, not technology
	â€¢	Case studies: Reference that all 11 wins in the presentation started with pain, not technology
	â€¢	Next steps: Monday Email exercise
SAMPLE RESPONSE:
Here's the good news: you don't need to understand AI to start. You need to understand where your people are suffering.
Every successful AI project I've seen in senior living started the same wayâ€”not with technology, but with a frustrated person and a painful workflow.
Let me give you your Monday morning action:
Send this email to your department heads: "What task makes your best person curse at their computer? What would you celebrate never doing again?"
The answers will tell you exactly where to focus.
Then, walk your buildings at 3 PM on a Friday. Whatever's piling upâ€”whatever people are rushing to finish or pushing to Mondayâ€”that's your AI opportunity list.
You're looking for what I call "Spreadsheet Heroes"â€”the scheduler maintaining 14 Excel files, the BOM who knows every payer rule by heart, the MDS nurse staying late to catch documentation gaps. Shadow them for two hours. Every time they copy/paste, cross-check, or switch between systemsâ€”that's an AI opportunity.
Quick question before I go further: What department gives you the most headaches right nowâ€”Clinical, Billing, Staffing, or Admissions?
FOLLOW-UP PATHS:
	â€¢	"Staffing is killing us" â†’ Pivot to call-off coverage, agency spend, ReadyShift case study
	â€¢	"Billing/denials are out of control" â†’ Pivot to prior auth, Part B, Glide case study
	â€¢	"I don't have time to do discovery" â†’ Offer the AI Opportunity Discovery Worksheet as a shortcut
	â€¢	"What's a Spreadsheet Hero?" â†’ Explain with examples, offer to help identify theirs
LEAD GENERATION OPPORTUNITY: Strong opportunity. Offer the AI Opportunity Discovery Worksheet as a leave-behind, or suggest a 30-minute discovery call to identify their top 3 opportunities together.

SCENARIO 2: "Help Me Evaluate Prior Authorizations for AI"
OPERATOR SAYS: "Our prior auth process is a nightmare. My clinical team spends hours on each one and we still get denials. Is this a good fit for AI?"
SITUATION ANALYSIS:
	â€¢	Stage: Evaluation â€” they've identified the pain, now assessing fit
	â€¢	What they actually need: Validation that this is AI-ready + understanding of what AI would actually do
	â€¢	What they're struggling with: Time per auth, denial rates, staff burnout, knowledge silos (one person knows all the payer rules)
	â€¢	Misconceptions: May think AI will "do" the prior auth automatically; may not understand Assistant vs. Agent distinction
IDEAL RESPONSE APPROACH:
	â€¢	Apply: 5 Attributes of AI-Ready Workflows, TRUST Framework, Assistant vs. Agent
	â€¢	Questions first: Yes â€” break down which part of prior auth is painful
	â€¢	Information: Score against 5 attributes, explain what AI actually does in this workflow
	â€¢	Case studies: PACadvocate, Glide (prior auth narrative drafting)
	â€¢	Next steps: Document current state, measure baseline
SAMPLE RESPONSE:
Prior auth is one of the highest-ROI AI opportunities in senior living. Let me walk you through whyâ€”and help you confirm it fits your situation.
First, let's score it against the 5 Attributes of an AI-Ready Workflow:
	â€¢	High Friction? Absolutelyâ€”gathering clinical documentation, matching it to payer rules, writing the narrative, tracking approvals. Multiple systems, multiple steps.
	â€¢	High Frequency? If you're doing these daily or multiple times per week, yes.
	â€¢	High Urgency? Every delay is lost revenue. Yes.
	â€¢	High Annoyance? I've never met clinical staff who enjoy writing prior auth narratives.
	â€¢	High Predictability? Payer rules follow patterns. Most auths follow similar structures.
That's 5 out of 5â€”this is textbook AI-ready.
Now, a question for you: Which part takes the longestâ€”gathering the documentation, writing the narrative, or chasing approvals?
The answer determines what type of AI you need. If it's narrative writing + documentation gathering, you're looking at an AI Assistantâ€”something that reads the clinical record, surfaces the relevant support, and drafts the justification for your team to review and submit.
One organization I worked with cut 45 minutes per admission and an hour per ADR using exactly this approach. The AI reads, extracts, and draftsâ€”your team verifies and submits.
FOLLOW-UP PATHS:
	â€¢	"It's all of itâ€”the whole process is broken" â†’ Walk through each step, identify which to automate first
	â€¢	"What about the denials/appeals?" â†’ Explain ADR drafting, reference Glide case study
	â€¢	"Can AI just handle it automatically?" â†’ Explain why this is an Assistant workflow (high risk requires human verification)
	â€¢	"What vendors do this?" â†’ Provide vendor evaluation criteria, reference green/red flags
LEAD GENERATION OPPORTUNITY: Strong. Offer to walk through the TRUST framework together to confirm whether they need an Assistant or Agent, and help them define success metrics for a pilot.

SCENARIO 3: "My IT Department Wants to Lead Our AI Initiative"
OPERATOR SAYS: "Our IT director is excited about AI and wants to take the lead on finding solutions. That makes sense, right? They handle all our technology."
SITUATION ANALYSIS:
	â€¢	Stage: Decision/Planning â€” they're figuring out organizational structure
	â€¢	What they actually need: A reframe on AI governance; AI is workflow transformation, not a software project
	â€¢	What they're struggling with: Organizational alignment, unclear ownership
	â€¢	Misconceptions: AI = technology = IT project. This is the most common failure pattern.
IDEAL RESPONSE APPROACH:
	â€¢	Apply: "Where AI Should Live" framework (Ops-Led + IT-Enabled)
	â€¢	Questions first: Understand current dynamic, IT's vision, Ops involvement
	â€¢	Information: Why AI must live in Ops, what happens when IT leads alone
	â€¢	Case studies: Reference pattern from all successful implementations
	â€¢	Next steps: Propose governance model
SAMPLE RESPONSE:
I appreciate the enthusiasm from ITâ€”that's actually a great sign. But here's the pattern I've seen across dozens of implementations:
When AI is led by IT alone, it almost always fails.
Here's why: AI in senior living isn't a software project. It's workflow transformation. It touches documentation, staffing, billing, admissions, survey prep, contract managementâ€”these are operational domains.
IT doesn't feel the pain of prior auth denials. They don't live in PCC. They don't manage the staffing matrix or sit through survey prep. They don't know which BOM is drowning in spreadsheets.
When AI becomes an IT project, here's what usually happens:
	â€¢	Operators aren't involved early enough
	â€¢	Nobody maps the real workflows
	â€¢	The tool ends up adding steps, not removing them
	â€¢	Staff doesn't adopt it
	â€¢	It becomes "digital shelfware"
	â€¢	Leadership concludes "AI didn't work for us"
The model that works: Ops leads, IT enables.
Ops identifies the workflow, owns the pilot, measures success, and drives adoption. IT ensures it's secure, maintains access, supports integration, and keeps systems stable.
Here's my suggestion: Keep your IT director excited and involvedâ€”you need them. But find an operational champion (a DON, BOM, or administrator) to own the first pilot. Let IT be the partner, not the driver.
Does that shift make sense for your organization?
FOLLOW-UP PATHS:
	â€¢	"How do I tell my IT director this without offending them?" â†’ Suggest framing as "we need you as a partner, not the sole owner"
	â€¢	"Our IT team is already evaluating vendors" â†’ Red flag; ask what workflows they're solving for
	â€¢	"Who should lead then?" â†’ Help identify the right operational champion
	â€¢	"What if Ops doesn't have bandwidth?" â†’ Discuss lightweight governance models
LEAD GENERATION OPPORTUNITY: Moderate. This is a strategic conversationâ€”offer to help design their AI governance structure or facilitate a kickoff meeting that includes both IT and Ops.

SCENARIO 4: "We Tried AI Before and It Failed"
OPERATOR SAYS: "We actually tried an AI tool about a year ago. It was supposed to help with documentation but nobody used it after the first month. I'm skeptical this time will be different."
SITUATION ANALYSIS:
	â€¢	Stage: Re-evaluation â€” burned before, cautious now
	â€¢	What they actually need: Post-mortem understanding of why it failed + a different approach
	â€¢	What they're struggling with: Skepticism, change fatigue, sunk cost anxiety
	â€¢	Misconceptions: May think "AI doesn't work for us" when the problem was implementation approach
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Failure Insurance framework, 5 Attributes, Common Mistakes to Avoid
	â€¢	Questions first: Yes â€” dig into what failed and why
	â€¢	Information: Diagnostic questions, common failure patterns
	â€¢	Case studies: Reference contrast between failed and successful implementations
	â€¢	Next steps: Proper discovery process
SAMPLE RESPONSE:
I appreciate you sharing thatâ€”and honestly, your skepticism is healthy. A lot of AI tools become shelfware. Let's figure out why that happened so we don't repeat it.
Can I ask a few diagnostic questions?
	â€¢	Did the tool add steps or remove steps? If it added workâ€”even one extra clickâ€”that's usually the death blow. AI should reduce friction, not create it.
	â€¢	Who owned the pilot? Was it driven by Ops (the people doing the work) or by IT/a vendor pushing a solution? If the end users weren't involved in selecting and shaping it, adoption almost always fails.
	â€¢	Was there a clear "before vs. after" metric? If success wasn't defined in terms of time saved, touches removed, or errors reduced, there was no way to prove valueâ€”and no motivation to stick with it.
	â€¢	Did it solve a problem people actually cared about? Sometimes vendors sell solutions to problems that aren't the real pain. Your nurses might have hated documentation, but maybe the AI solved the wrong part of documentation.
Here's what I've learned: AI failure is rarely about the technology. It's about:
	â€¢	Picking the wrong workflow
	â€¢	Not involving the right people
	â€¢	Adding steps instead of removing them
	â€¢	Piloting too broadly
	â€¢	No clear success metrics
If you're willing to try again, I'd suggest starting with a proper discovery processâ€”find the workflow that's truly high-friction, high-frequency, and high-annoyance. Then pilot with ONE team, ONE building, and ONE defined metric.
What was the workflow the failed tool was supposed to address?
FOLLOW-UP PATHS:
	â€¢	"It was supposed to help with [specific workflow]" â†’ Evaluate that workflow against 5 Attributes
	â€¢	"I don't rememberâ€”we just bought what the vendor sold us" â†’ Red flag; emphasize workflow-first approach
	â€¢	"How is this time different?" â†’ Walk through the structured discovery + pilot methodology
	â€¢	"My team won't trust AI now" â†’ Address change management, suggest starting with biggest skeptic
LEAD GENERATION OPPORTUNITY: Strong. Offer a "pilot post-mortem" session to diagnose what went wrong, or help them run a fresh discovery process with proper methodology.

SCENARIO 5: "A Vendor is Promising Everythingâ€”How Do I Evaluate Them?"
OPERATOR SAYS: "I just got off a demo with a vendor who says their AI will transform our entire operationâ€”documentation, billing, scheduling, everything. It sounds amazing but also too good to be true. How do I know if they're legit?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision â€” actively evaluating, needs BS detector
	â€¢	What they actually need: Vendor evaluation framework, red/green flags
	â€¢	What they're struggling with: Distinguishing hype from substance, fear of making expensive mistake
	â€¢	Misconceptions: May believe "more features = better solution"; may not realize they should pick workflow before tool
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Vendor Green Flags / Red Flags, Assistant vs. Agent clarity
	â€¢	Questions first: What specific workflow are they trying to solve?
	â€¢	Information: Vendor evaluation criteria
	â€¢	Case studies: Contrast with how successful tools were selected
	â€¢	Next steps: Define workflow first, then re-evaluate vendor
SAMPLE RESPONSE:
Your instinct is rightâ€”when a vendor promises to "transform everything," that's usually a red flag.
Here's my vendor BS detector:
ðŸš© Red Flags (run away if you hear):
	â€¢	"Our AI will transform everything"
	â€¢	"No pilot neededâ€”just roll it out"
	â€¢	"It works for everyone"
	â€¢	They can't provide references at facilities your size
	â€¢	They can't explain exactly what the AI does (vs. what humans do)
	â€¢	The solution adds steps to your workflow
	â€¢	They're pushing enterprise pricing before you've tested anything
âœ… Green Flags (good signs):
	â€¢	"Let's start with a 30-60 day pilot in one department"
	â€¢	"Here are 3 facilities like yours using it"
	â€¢	"It will take 30-60 days to see results"
	â€¢	They're clear about what AI handles vs. what your team handles
	â€¢	They admit what it can't do
	â€¢	They ask about your specific workflows before pitching features
Here's the fundamental question: Have you already identified the specific workflow you want to fix?
The biggest mistake I see is picking tools before picking workflows. That vendor demo is showing you 20 features, but which one solves the problem that's actually causing pain in your buildings?
Before your next conversation with them, I'd suggest:
	â€¢	Identify your #1 pain point workflow
	â€¢	Ask them specifically: "How does your tool handle [that exact workflow]?"
	â€¢	Ask for a reference at a similar-sized organization who implemented that specific use case
	â€¢	Insist on a pilot before enterprise commitment
What workflow were you hoping they'd solve?
FOLLOW-UP PATHS:
	â€¢	"They said they do prior auth/scheduling/documentation" â†’ Evaluate that specific claim against reality
	â€¢	"I don't have a specific workflowâ€”I was hoping they'd tell me" â†’ Major red flag; redirect to discovery process
	â€¢	"What questions should I ask them?" â†’ Provide specific vendor interview questions
	â€¢	"They want a 3-year contract" â†’ Strongly advise against; insist on pilot first
LEAD GENERATION OPPORTUNITY: Strong. Offer to join their next vendor call as an advisor, or provide a "Vendor Evaluation Scorecard" they can use.

SCENARIO 6: "Should I Use an Assistant or Agent for [Specific Workflow]?"
OPERATOR SAYS: "I've heard you talk about Assistants vs. Agents. We're looking at automating our referral intake process for after-hours admissions. Which one do we need?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision â€” understands the framework, applying it to a specific case
	â€¢	What they actually need: TRUST framework application to their specific workflow
	â€¢	What they're struggling with: Classification uncertainty, risk assessment
	â€¢	Misconceptions: May think Agent = better/more advanced; may not realize it's about context, not capability
IDEAL RESPONSE APPROACH:
	â€¢	Apply: TRUST Framework, Assistant vs. Agent decision tree
	â€¢	Questions first: Walk through TRUST questions for their workflow
	â€¢	Information: Classification logic with rationale
	â€¢	Case studies: Similar workflows with clear classifications
	â€¢	Next steps: Document requirements, evaluate tools
SAMPLE RESPONSE:
Great questionâ€”and after-hours referral intake is a perfect example of where this decision really matters.
Let me walk you through the TRUST framework for your situation:
T - Time Sensitivity: When a referral comes in at 2 AM, how fast must you respond?
	â€¢	If the hospital expects a response in minutes and no one's available â†’ You need an Agent
	â€¢	If it can wait until morning â†’ Assistant might work
R - Risk Tolerance: What happens if AI makes a wrong callâ€”screens out a good admit or accepts a bad one?
	â€¢	If a mistake is catastrophic (compliance, care, major revenue loss) â†’ You need an Assistant (human verification)
	â€¢	If mistakes are fixable â†’ Agent could work
U - Uniformity: What percentage of after-hours referrals follow the same pattern?
	â€¢	If 80%+ are straightforward screens â†’ Agent can handle most
	â€¢	If highly variable â†’ Need human judgment (Assistant)
S - Staff Capacity: Is someone always available to review after-hours referrals?
	â€¢	If no one's monitoring overnight â†’ You need an Agent to act independently
	â€¢	If you have on-call staff â†’ Assistant could support them
T - Training Investment: How fast do you need results?
	â€¢	Need wins this month â†’ Start with Assistant (faster to implement)
	â€¢	Willing to invest 3+ months â†’ Agent might be worth it
Based on typical after-hours referral patterns:
	â€¢	Time is critical (hospitals expect fast response)
	â€¢	Risk is moderate (can be reviewed next morning)
	â€¢	Process is fairly uniform (insurance verification, bed availability)
	â€¢	Staff typically not available at 2 AM
My read: You likely need an Agent for initial response + qualification, with human review for final acceptance.
Does that match your situation, or should we adjust any of those assumptions?
FOLLOW-UP PATHS:
	â€¢	"We do have on-call staff, but they're overwhelmed" â†’ Agent to handle volume, escalate exceptions
	â€¢	"What would the Agent actually do?" â†’ Walk through specific actions: acknowledge referral, pull insurance info, check bed availability, send initial response
	â€¢	"What if it makes a mistake?" â†’ Discuss escalation protocols, human-in-the-loop safeguards
	â€¢	"What tools do this?" â†’ Discuss options, reference Glide's referral components
LEAD GENERATION OPPORTUNITY: Moderate. Offer to map out their complete referral workflow and design the human + AI handoff points.

SCENARIO 7: "My Team is Scared AI Will Replace Them"
OPERATOR SAYS: "I brought up AI at our staff meeting and the room went silent. My scheduler asked if she should start looking for another job. How do I get my team on board when they think they're being replaced?"
SITUATION ANALYSIS:
	â€¢	Stage: Pre-implementation â€” facing change management barrier
	â€¢	What they actually need: Messaging framework, reassurance narrative, proof points
	â€¢	What they're struggling with: Staff fear, morale concerns, trust deficit
	â€¢	Misconceptions: Staff (and possibly operator) may believe AI = automation = job cuts
IDEAL RESPONSE APPROACH:
	â€¢	Apply: "Load reduction, not job replacement" messaging, demographic crisis context
	â€¢	Questions first: Understand specific fears, identify the "Marys" on their team
	â€¢	Information: Reframing narrative, examples of AI giving time back
	â€¢	Case studies: Any case where staff loved the AI outcome
	â€¢	Next steps: Involve skeptics in pilot, start with their biggest annoyance
SAMPLE RESPONSE:
That reaction is completely normalâ€”and honestly, it's a sign your team cares about their jobs. Let me give you language that might help.
Here's the reframe I use:
"AI isn't replacing people. AI is reducing the workload per personâ€”so we can keep up with the demographic wave that's already here."
The math is simple: The 85+ population will double. The working-age population is flat. We're already short hundreds of thousands of caregivers. By the end of the decade, that shortage could exceed one million.
We cannot hire our way out of this.
Your scheduler isn't going to lose her job. But right now, how many texts does she send when someone calls off? How many hours does she spend juggling last-minute coverage? What would her week look like if AI handled the initial outreach and she only had to approve the final shift fills?
Here's my suggestion: Find your scheduler's biggest annoyanceâ€”the task she absolutely hatesâ€”and pilot AI on that. When she sees AI take away the work she dreads (not the work she values), she becomes your champion.
The message to your team:
"We're not using AI to cut staff. We're using AI to stop drowning you in paperwork, texts, and busywork. Our goal is to give you back time for the work that actually mattersâ€”and that only you can do."
One more tip: Start your pilot with your biggest skeptic. When they become a believer, everyone else follows.
Who on your team holds the most pain right now?
FOLLOW-UP PATHS:
	â€¢	"My scheduler/BOM/DON is the most overwhelmed" â†’ Identify their specific workflow, propose shadowing
	â€¢	"How do I message this to the whole organization?" â†’ Provide talking points for all-staff communication
	â€¢	"What if they still resist?" â†’ Discuss involving them in tool selection, showing them the "before/after"
	â€¢	"We actually are overstaffed and might cut positions" â†’ Different conversationâ€”be honest about that separately from AI
LEAD GENERATION OPPORTUNITY: Moderate. Offer change management talking points or a "staff communication template" for AI rollouts.

SCENARIO 8: "We're Too Small for AI" (50-bed facility)
OPERATOR SAYS: "All these AI examples are from big organizations with lots of resources. We're a single 50-bed ALF. Can AI actually help us, or is this only for the big players?"
SITUATION ANALYSIS:
	â€¢	Stage: Discovery/Evaluation â€” questioning fit
	â€¢	What they actually need: Examples scaled to their size, low-barrier entry points
	â€¢	What they're struggling with: Resource constraints, economies of scale concerns
	â€¢	Misconceptions: AI requires enterprise investment; small facilities can't benefit
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Off-the-shelf tools (Otter, Limitless, AI documentation assistants), simple wins
	â€¢	Questions first: Understand their specific pain points at scale
	â€¢	Information: Tools that work at any size, no-code solutions
	â€¢	Case studies: Examples of single-facility or small org wins
	â€¢	Next steps: Identify one workflow, one free/low-cost tool
SAMPLE RESPONSE:
Actually, small facilities are often the best place for AIâ€”and I'll tell you why.
In a 50-bed ALF, you probably have:
	â€¢	Fewer layers of approval to navigate
	â€¢	Faster decision-making
	â€¢	Staff who wear multiple hats (and feel the pain directly)
	â€¢	An administrator who can champion something personally
The big organizations I work with often struggle with bureaucracy, IT committees, and vendor procurement that takes 18 months. You can try something next week.
Here's the truth: You don't need enterprise AI. You need simple tools that work.
Tools you can use todayâ€”many are free or low-cost:
	â€¢	Otter.ai â€” Transcribe your IDT meetings and get automatic summaries
	â€¢	AI documentation assistants â€” Draft shift notes, ADL summaries, family updates
	â€¢	Claude or ChatGPT â€” Answer regulatory questions instantly (you can even load your state's ALF regs and ask it anything)
	â€¢	AI inbox triage â€” Summarize emails, prioritize what needs attention
None of these require enterprise contracts, IT departments, or huge budgets.
Here's my question for you: What's the single most annoying recurring task in your building? The thing you or your team does every single day that feels like pure overhead?
Let's find one simple AI tool that makes that task 50% faster. That's your proof point. Everything else builds from there.
FOLLOW-UP PATHS:
	â€¢	"I spend hours on survey prep documentation" â†’ Point to ALF Reg Chatbot example, offer to help set one up
	â€¢	"My biggest problem is staffing" â†’ Discuss scheduling tools, even simple automation
	â€¢	"I don't even know what tools exist" â†’ Provide curated list of small-facility-friendly AI tools
	â€¢	"What would it cost?" â†’ Walk through free/low-cost options first
LEAD GENERATION OPPORTUNITY: Moderate. Offer a "Small Facility AI Starter Kit" or a quick call to identify their best first tool.

SCENARIO 9: "We Don't Have Budget for AI"
OPERATOR SAYS: "I get that AI could help, but we're barely making our numbers as it is. There's no way leadership will approve budget for AI tools when we're cutting everywhere else."
SITUATION ANALYSIS:
	â€¢	Stage: Discovery/Evaluation â€” blocked by perceived resource constraint
	â€¢	What they actually need: ROI framing, "pays for itself" examples, free/low-cost starting points
	â€¢	What they're struggling with: Budget pressure, competing priorities, proving value before investment
	â€¢	Misconceptions: AI requires significant upfront investment; AI is a cost center, not a cost saver
IDEAL RESPONSE APPROACH:
	â€¢	Apply: $100K Rule, ROI calculations, cost-elimination case studies
	â€¢	Questions first: Where are they spending money on manual work, agency, rework, or outsourcing?
	â€¢	Information: Examples of AI that paid for itself in 30-60 days
	â€¢	Case studies: PCC Automation ($20K/month eliminated), Part B Billing (90% cost savings), ReadyShift (agency reduction)
	â€¢	Next steps: Identify a cost-elimination target
SAMPLE RESPONSE:
I hear youâ€”and honestly, "we don't have budget" is often the exact reason you need AI.
Here's how I think about it: AI isn't an expense line. It's a cost-elimination tool.
Let me give you real examples:
PCC Automation: One organization was paying $125 per new employee to an outsourced helpdesk just to create PCC logins. At 130 new employees per month, that's $16,250/monthâ€”$195,000/year. AI-designed automation eliminated it entirely. Zero ongoing cost.
Part B Billing: Another was spending $2M/year outsourcing Part B claims. AI-assisted coding let them bring it in-houseâ€”estimated 90% savings.
Agency Spend: ReadyShift is positioned to cut 30-50% off an $8M annual agency bill. That's $2.4-$4M saved.
The question isn't "Can we afford AI?"
The question is: "What are we spending money on that AI could eliminate?"
Let me ask you:
	â€¢	Are you outsourcing any billing, coding, or administrative functions?
	â€¢	What's your annual agency spend?
	â€¢	How many hours per week do staff spend on tasks a robot could do?
If we can find ONE workflow where AI eliminates $100K+ in cost within 12 months, that becomes a self-funding pilot. Leadership loves projects that pay for themselves.
Want to map your cost-elimination opportunities?
FOLLOW-UP PATHS:
	â€¢	"We outsource [specific function]" â†’ Calculate potential savings, propose in-sourcing with AI
	â€¢	"Our agency spend is huge" â†’ Discuss staffing marketplace options, calculate ROI
	â€¢	"What about free tools?" â†’ Redirect to low-cost starting points (Otter, ChatGPT, etc.)
	â€¢	"How do I pitch this to leadership?" â†’ Provide business case framework (Scenario 10/12)
LEAD GENERATION OPPORTUNITY: Strong. Offer to build a cost-elimination analysis or ROI model they can present to leadership.

SCENARIO 10: "How Do I Convince Leadership to Invest in AI?"
OPERATOR SAYS: "I'm bought in on AI, but my CEO and CFO are skeptical. They want to see numbers before they'll approve anything. How do I build a case they'll actually support?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision â€” internal advocacy, needs ammunition
	â€¢	What they actually need: Business case framework, executive-friendly language, proof points
	â€¢	What they're struggling with: Credibility gap, competing priorities, executive skepticism
	â€¢	Misconceptions: May think they need to explain AI technology; actually need to explain ROI
IDEAL RESPONSE APPROACH:
	â€¢	Apply: $100K Rule, 5 Impact Categories, cost-elimination framing
	â€¢	Questions first: What does leadership care about most? Revenue, cost, compliance, retention?
	â€¢	Information: Business case structure, metrics that matter
	â€¢	Case studies: Specific ROI examples with numbers
	â€¢	Next steps: Build one-page business case
SAMPLE RESPONSE:
Good news: You don't need to convince leadership that AI is cool. You need to convince them it makes money (or saves money).
Here's the framework that works:
Your pilot must impact at least ONE of these:
	â€¢	Increase revenue (faster admissions, fewer denials, better PDPM capture)
	â€¢	Decrease cost (eliminate outsourcing, reduce agency, cut overtime)
	â€¢	Improve patient outcomes (fewer falls, better care transitions)
	â€¢	Improve employee experience (reduce burnout, improve retention)
	â€¢	Reduce compliance risk (fewer survey tags, better documentation)
Then apply the $100K Rule: If the workflow can't realistically return $100K in the first 12 months, pick a different workflow.
Here's how I'd structure your pitch:
Page 1: The Problem
	â€¢	Specific workflow, specific pain, specific cost
	â€¢	"We spend X hours per week on [workflow]. That's Y dollars in labor, plus Z in errors/denials/delays."
Page 2: The Solution
	â€¢	What AI would do (be specificâ€”not "transform our operations")
	â€¢	What humans would still do
	â€¢	Pilot scope: ONE building, ONE workflow, 60 days
Page 3: The Numbers
	â€¢	Conservative ROI projection
	â€¢	"At 50% improvement, we save $X in Year 1"
	â€¢	Pilot cost vs. projected savings
Page 4: The Ask
	â€¢	Pilot budget and timeline
	â€¢	Success criteria
	â€¢	Decision point (scale, pivot, or kill)
What's the specific workflow you want to pilot? Let's build the numbers together.
FOLLOW-UP PATHS:
	â€¢	"My CFO only cares about cost" â†’ Focus on cost-elimination examples (PCC, outsourcing)
	â€¢	"Our CEO cares about resident care" â†’ Frame as "giving staff time back for care"
	â€¢	"They'll want to see other facilities doing this" â†’ Provide case studies with metrics
	â€¢	"Can you help me build the actual presentation?" â†’ Offer to draft their one-pager
LEAD GENERATION OPPORTUNITY: Strong. Offer to co-build their executive presentation or provide a "Leadership Pitch Template."

SCENARIO 11: "What's the Fastest Win We Can Get?"
OPERATOR SAYS: "My boss gave me 60 days to show results with AI or we're moving on to other priorities. What's the quickest win I can get?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision/Implementation â€” urgency-driven, needs low-risk fast win
	â€¢	What they actually need: Pre-validated, low-friction opportunity they can execute immediately
	â€¢	What they're struggling with: Time pressure, proving value quickly, avoiding complexity
	â€¢	Misconceptions: May think they need custom solutions; actually need off-the-shelf quick wins
IDEAL RESPONSE APPROACH:
	â€¢	Apply: 5 Attributes, "already possible today" tools, simple automation patterns
	â€¢	Questions first: Minimal â€” they need action, not extensive discovery
	â€¢	Information: Pre-vetted quick wins by department
	â€¢	Case studies: PCC Automation, meeting transcription, documentation assistants
	â€¢	Next steps: Pick one, implement this week
SAMPLE RESPONSE:
Sixty days? Let's get you a win in two weeks, not eight.
Here are the fastest wins I've seen, by department:
If your pain is in STAFFING:
	â€¢	Set up an automated call-off notification system (many scheduling tools have this)
	â€¢	Use a group text tool with templates instead of individual messages
	â€¢	Win: Scheduler saves 30+ minutes per call-off
If your pain is in DOCUMENTATION:
	â€¢	Pilot Otter.ai for your IDT or QAPI meetingsâ€”automatic transcription + summary
	â€¢	Try an AI documentation assistant for shift notes (many work inside EHRs now)
	â€¢	Win: Nurses save 30+ minutes per shift
If your pain is in MEETINGS:
	â€¢	Start recording every meeting with AI transcription
	â€¢	Generate action items automatically
	â€¢	Win: No more "who said what" confusion
If your pain is in ADMIN:
	â€¢	Identify your most manual recurring task (PCC logins, report generation, data entry)
	â€¢	Build a simple automation (often with no-code tools)
	â€¢	Win: Hours eliminated per week
The fastest win is usually the smallest workflow with the highest annoyance. Not the biggest problemâ€”the most frustrating one.
Quick question: What task makes someone on your team groan every single day?
FOLLOW-UP PATHS:
	â€¢	"Documentation is our biggest pain" â†’ Recommend specific AI note assistant, help them set up
	â€¢	"We waste hours in meetings" â†’ Walk through Otter.ai or similar setup
	â€¢	"I need to show numbers to my boss" â†’ Help define success metric for their chosen workflow
	â€¢	"Can we do something bigger?" â†’ Advise starting small; bigger wins follow proven patterns
LEAD GENERATION OPPORTUNITY: Moderate. Offer to help them set up their first tool or define their success metrics.

SCENARIO 12: "Help Me Build a Business Case for AI"
OPERATOR SAYS: "I need to present a formal business case to our executive team next month. Can you help me structure it? I've never done something like this for AI before."
SITUATION ANALYSIS:
	â€¢	Stage: Decision â€” needs to formalize and present opportunity
	â€¢	What they actually need: Complete business case structure with their numbers
	â€¢	What they're struggling with: Framework, credibility, projections
	â€¢	Misconceptions: May try to make it about AI technology rather than business outcomes
IDEAL RESPONSE APPROACH:
	â€¢	Apply: $100K Rule, 5 Impact Categories, cost-elimination framing
	â€¢	Questions first: What workflow, what metrics do they have, what does leadership care about?
	â€¢	Information: Business case template, section-by-section guidance
	â€¢	Case studies: Reference specific ROI numbers from presentation
	â€¢	Next steps: Complete each section with their data
SAMPLE RESPONSE:
Absolutelyâ€”let's build this together. A strong AI business case has five sections:
SECTION 1: THE PROBLEM (Why This Matters)
	â€¢	What specific workflow are you targeting?
	â€¢	How much time does it consume? (hours/week)
	â€¢	How many people does it affect?
	â€¢	What's the cost of the current state? (labor, errors, delays, outsourcing)
	â€¢	What's the strategic risk of not solving it?
SECTION 2: THE OPPORTUNITY (Why AI Fits)
	â€¢	Why is this workflow AI-ready? (Score against 5 Attributes)
	â€¢	What type of AI is needed? (Assistant or Agent)
	â€¢	What would AI actually do vs. what humans would still do?
SECTION 3: THE SOLUTION (What We'll Do)
	â€¢	Specific tool or approach (if known)
	â€¢	Pilot scope: ONE building, ONE workflow, 60-90 days
	â€¢	Who owns it (operational champion, not IT)
SECTION 4: THE NUMBERS (Why It Pays Off)
	â€¢	Projected time savings
	â€¢	Projected cost reduction or revenue increase
	â€¢	Conservative vs. optimistic scenarios
	â€¢	Apply the $100K Rule: Can this return $100K in Year 1?
SECTION 5: THE ASK (What We Need)
	â€¢	Pilot budget
	â€¢	Timeline
	â€¢	Success criteria (specific metrics)
	â€¢	Decision point: Scale, pivot, or kill
Let's start with Section 1. What workflow are you proposing to fix, and do you have baseline metrics on time spent and people involved?
FOLLOW-UP PATHS:
	â€¢	"I don't have good baseline metrics" â†’ Guide them on how to gather (shadow the hero, time studies)
	â€¢	"I'm not sure which workflow to choose" â†’ Step back to discovery/prioritization
	â€¢	"Can you help me calculate the ROI?" â†’ Walk through granular calculation (like 5 Silent Killers examples)
	â€¢	"I need to make it look professional" â†’ Offer template or formatting guidance
LEAD GENERATION OPPORTUNITY: Strong. Offer to review their completed draft, help with calculations, or provide a polished template.

SCENARIO 13: "We Have 10 Ideasâ€”Which Should We Prioritize?"
OPERATOR SAYS: "My team brainstormed AI opportunities and we came up with a list of 10 things. But I can't do them all at once. How do I figure out which one to start with?"
SITUATION ANALYSIS:
	â€¢	Stage: Discovery/Decision â€” needs prioritization framework
	â€¢	What they actually need: Scoring methodology to rank opportunities objectively
	â€¢	What they're struggling with: Too many options, analysis paralysis, fear of wrong choice
	â€¢	Misconceptions: May want to pick the "biggest" problem; should pick the best-fit problem
IDEAL RESPONSE APPROACH:
	â€¢	Apply: 5 Attributes scoring, $100K Rule, prioritization matrix
	â€¢	Questions first: Get the list, gather basic data on each
	â€¢	Information: Scoring framework
	â€¢	Case studies: How winning projects were selected
	â€¢	Next steps: Score all 10, pick top 3, apply TRUST to #1
SAMPLE RESPONSE:
This is actually the perfect problem to haveâ€”you've done the hard work of discovery. Now let's prioritize.
Here's the scoring method I use:
For each of your 10 opportunities, score three factors:
Frequency:
	â€¢	Daily = 3 points
	â€¢	Weekly = 2 points
	â€¢	Monthly = 1 point
Pain Level:
	â€¢	High (everyone hates it) = 3 points
	â€¢	Medium (it's annoying) = 2 points
	â€¢	Low (it's fine) = 1 point
People Affected:
	â€¢	Many (whole department+) = 3 points
	â€¢	Some (a few people) = 2 points
	â€¢	Few (one person) = 1 point
Total Score = Frequency Ã— Pain Ã— People
Maximum possible: 27 (daily, high pain, many people)
Your top 3 scoring opportunities are your finalists.
Then, for your #1 finalist, apply the $100K Rule:
	â€¢	Can this workflow realistically generate $100K in ROI within 12 months?
	â€¢	If yes â†’ This is your pilot
	â€¢	If no â†’ Move to your #2 finalist
Want to walk through your list together? Give me your 10 opportunities and any data you have on frequency, pain, and people affected.
FOLLOW-UP PATHS:
	â€¢	Operator lists their 10 opportunities â†’ Score each, rank, identify top 3
	â€¢	"Two scored the sameâ€”how do I choose?" â†’ Apply TRUST framework as tiebreaker
	â€¢	"The highest-scoring one seems risky" â†’ Discuss risk tolerance, consider #2 as starting point
	â€¢	"Can we do two at once?" â†’ Strongly advise against; one pilot at a time builds trust
LEAD GENERATION OPPORTUNITY: Strong. Offer to facilitate a prioritization session or review their scored list.

SCENARIO 14: "Our Pilot Isn't Workingâ€”What Do We Do?"
OPERATOR SAYS: "We started an AI pilot six weeks ago for documentation, but honestly it's not going well. Staff are frustrated and we're not seeing the time savings we expected. Should we just kill it?"
SITUATION ANALYSIS:
	â€¢	Stage: Implementation â€” pilot in trouble, needs triage
	â€¢	What they actually need: Diagnostic framework, decision criteria for kill/pivot/persist
	â€¢	What they're struggling with: Unclear on what went wrong, sunk cost pressure, team morale
	â€¢	Misconceptions: May think pilot failure = AI doesn't work; usually means wrong approach
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Failure Insurance framework, kill/pivot/scale criteria
	â€¢	Questions first: Diagnose what's actually happening
	â€¢	Information: Common failure patterns, decision tree
	â€¢	Case studies: Examples of pivots that worked
	â€¢	Next steps: Decide kill/pivot/persist based on data
SAMPLE RESPONSE:
First, let's diagnose before we decide. A struggling pilot usually has one of five root causes:
1. Wrong workflow: Is this actually high-friction, high-frequency, and predictable? If the workflow itself isn't AI-ready, no tool will help.
2. Wrong tool: Does the tool actually remove steps, or did it add steps? If staff have to do more work (even one extra click), adoption dies.
3. Wrong scope: Are you piloting with too many people, too many buildings, or too many variations of the workflow?
4. Wrong owner: Is the pilot being driven by someone who actually does the work? Or was it imposed from above?
5. Wrong metrics: Did you define "success" clearly before starting? Staff may feel it's failing when actually it's working but unnoticed.
Can you tell me:
	â€¢	What specifically is staff frustrated about?
	â€¢	Are they using the tool and it's not helping, or are they not using it at all?
	â€¢	Is the tool adding steps or removing them?
	â€¢	How were you measuring success?
Here's my decision framework:
Kill the pilot if:
	â€¢	It adds steps (not just fails to remove themâ€”actually adds work)
	â€¢	The workflow itself isn't AI-ready
	â€¢	Staff actively hate it and can't articulate a path forward
Pivot the pilot if:
	â€¢	The workflow is right but the tool isn't
	â€¢	The scope was too broadâ€”narrow to one team/building
	â€¢	Training was insufficientâ€”try retraining with better support
Persist with adjustments if:
	â€¢	Early friction is normal adoption curve
	â€¢	Metrics are improving even if slowly
	â€¢	Staff have specific, fixable complaints
What are you seeing in your pilot?
FOLLOW-UP PATHS:
	â€¢	"Staff say it takes longer than the old way" â†’ Tool is adding stepsâ€”consider killing or pivoting to different tool
	â€¢	"They're just not using it" â†’ Adoption issue; discuss training, champion involvement
	â€¢	"I'm not sure how we're measuring success" â†’ Fundamental problem; define metrics and reset
	â€¢	"The tool works but staff hate the change" â†’ Change management issue; involve biggest skeptic
LEAD GENERATION OPPORTUNITY: Strong. Offer a "pilot triage" session to diagnose and create an action plan.

SCENARIO 15: "How Long Should Our Pilot Take?"
OPERATOR SAYS: "We're about to start an AI pilot for our billing process. How long should we run it before we know if it's working? I've heard everything from 2 weeks to 6 months."
SITUATION ANALYSIS:
	â€¢	Stage: Implementation â€” planning phase, needs timeline guidance
	â€¢	What they actually need: Realistic timeline framework based on workflow type
	â€¢	What they're struggling with: Uncertainty about evaluation timeframe
	â€¢	Misconceptions: One-size-fits-all timeline; actually depends on workflow frequency
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Pilot cadence framework, frequency-based timeline logic
	â€¢	Questions first: What specific billing workflow? How often does it occur?
	â€¢	Information: Timeline guidance based on volume
	â€¢	Case studies: How long successful pilots ran
	â€¢	Next steps: Define specific milestones and checkpoints
SAMPLE RESPONSE:
The answer depends on workflow frequencyâ€”you need enough volume to see patterns.
Here's my timeline framework:
Daily workflows (documentation, call-offs, census updates):
	â€¢	Pilot duration: 30-45 days
	â€¢	You'll see patterns within 2 weeks
	â€¢	Give 2-3 more weeks to confirm they hold
Weekly workflows (PDPM audits, Key Factor prep, therapy reviews):
	â€¢	Pilot duration: 60-90 days
	â€¢	You need 8-12 cycles to see real patterns
	â€¢	First month is often just learning curve
Monthly workflows (month-end billing, contract reviews, compliance audits):
	â€¢	Pilot duration: 90-120 days
	â€¢	You need 3+ cycles minimum
	â€¢	Don't judge until after at least 2 full cycles
For your billing workflow, my questions are:
	â€¢	Is this daily billing tasks, weekly batch processing, or monthly close?
	â€¢	How many "transactions" per week will flow through the AI?
For billing specifically, I typically recommend:
	â€¢	60-90 days as baseline
	â€¢	Weekly check-ins (20 minutes) to review 5-10 examples
	â€¢	Clear metrics defined upfront (time per claim, denial rate, touches reduced)
	â€¢	Decision point at 60 days: scale, pivot, or kill
What's the rhythm of the specific billing process you're targeting?
FOLLOW-UP PATHS:
	â€¢	"It's prior authâ€”we do 10-20 per day" â†’ 30-45 day pilot is sufficient
	â€¢	"It's month-end reconciliation" â†’ Need 90+ days to see 3 cycles
	â€¢	"What should we track during the pilot?" â†’ Lead to Scenario 16 (metrics)
	â€¢	"What if we're not seeing results halfway through?" â†’ Reference Scenario 14 (pilot triage)
LEAD GENERATION OPPORTUNITY: Moderate. Offer a pilot planning template or milestone checklist.

SCENARIO 16: "What Metrics Should We Track?"
OPERATOR SAYS: "We're starting an AI pilot for scheduling next week. What metrics should we track to know if it's actually working?"
SITUATION ANALYSIS:
	â€¢	Stage: Implementation â€” pilot setup, needs measurement framework
	â€¢	What they actually need: Specific, relevant metrics for their workflow type
	â€¢	What they're struggling with: Defining success, avoiding vanity metrics
	â€¢	Misconceptions: May focus on technology metrics; need operational metrics
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Impact categories (time, touches, errors, satisfaction), baseline measurement
	â€¢	Questions first: What does success look like in operational terms?
	â€¢	Information: Metric categories with specific examples
	â€¢	Case studies: How successful pilots measured results
	â€¢	Next steps: Establish baseline before starting
SAMPLE RESPONSE:
Great questionâ€”and critical to define BEFORE you start, not after.
For scheduling, here are the metrics that matter:
TIME METRICS:
	â€¢	Minutes per call-off resolution (from notification to shift filled)
	â€¢	Total hours/week spent on scheduling tasks
	â€¢	Time spent texting/calling vs. other work
TOUCH METRICS:
	â€¢	Number of texts/calls per shift fill
	â€¢	Number of manual system entries
	â€¢	Handoffs between people per scheduling change
OUTCOME METRICS:
	â€¢	Open shift fill rate (what % get filled, and how fast?)
	â€¢	Agency utilization rate (are you still hitting agencies?)
	â€¢	Overtime hours
SATISFACTION METRICS:
	â€¢	Staff satisfaction with scheduling process (simple 1-5 weekly survey)
	â€¢	Scheduler stress level
	â€¢	The "would you go back?" question at pilot end
Here's the critical step: Measure your baseline BEFORE you start.
This week, have your scheduler track:
	â€¢	How long each call-off takes to resolve
	â€¢	How many texts/calls per shift fill
	â€¢	Current open shift fill rate
	â€¢	Current agency vs. internal fill ratio
Then track the same metrics weekly during the pilot. The comparison tells the story.
One warning: Don't track too many metrics. Pick 3-4 that matter most. For scheduling, I'd recommend:
	â€¢	Time per call-off resolution (efficiency)
	â€¢	Messages per shift fill (friction)
	â€¢	Agency fill rate (cost impact)
	â€¢	Scheduler weekly satisfaction score (human impact)
Does that work for your situation, or should we adjust for your specific context?
FOLLOW-UP PATHS:
	â€¢	"How do I track these without overwhelming my scheduler?" â†’ Suggest simple tracking methods, weekly time-box
	â€¢	"What if metrics are improving but my team hates it?" â†’ Satisfaction trumps efficiency; dig into why
	â€¢	"We don't have good baseline data" â†’ Spend first week measuring baseline before going live
	â€¢	"What's a good target for improvement?" â†’ Generally aim for 25-50% improvement in primary metric
LEAD GENERATION OPPORTUNITY: Moderate. Offer a metrics tracking template or weekly pilot review agenda.

SCENARIO 17: "Can AI Help with [Clinical Workflow]?" (When Answer is Cautious)
OPERATOR SAYS: "What about using AI for fall risk assessments? We have residents falling and I'm wondering if AI could help us predict and prevent falls better."
SITUATION ANALYSIS:
	â€¢	Stage: Discovery/Evaluation â€” asking about clinical AI
	â€¢	What they actually need: Honest guidance on clinical AI readiness; redirect to admin wins first
	â€¢	What they're struggling with: Genuine clinical challenge, but may not realize clinical AI is higher-risk
	â€¢	Misconceptions: AI is equally applicable to all workflows; clinical workflows have different risk profiles
IDEAL RESPONSE APPROACH:
	â€¢	Apply: TRUST framework (Risk component), "start with admin" principle, clinical AI maturity assessment
	â€¢	Questions first: Understand their clinical workflow, current prevention approach
	â€¢	Information: Why clinical AI is different, what's possible vs. what's prudent
	â€¢	Case studies: Fall detection with human alert (Agent + human escalation)
	â€¢	Next steps: Evaluate carefully, consider admin wins first
SAMPLE RESPONSE:
Fall prevention is a real problem and AI can potentially helpâ€”but I want to give you a nuanced answer, not a sales pitch.
Here's how I think about clinical AI:
The TRUST question that matters most: Risk Tolerance.
For falls, if AI makes a wrong predictionâ€”misses a high-risk resident or falsely flags a low-risk oneâ€”the consequences can be serious. Injury, survey citations, family complaints, liability.
Clinical workflows require higher certainty before AI acts independently. That doesn't mean AI can't helpâ€”it means we approach it differently.
What's possible today:
	â€¢	AI-assisted fall risk scoring â€” AI can analyze resident data, identify patterns, and flag high-risk residents for nursing review. This is an Assistant approachâ€”AI advises, humans decide.
	â€¢	Sensor-based fall detection â€” AI monitors motion sensors and alerts staff when a fall may have occurred. This is an Agent approachâ€”AI acts (sends alert), but the action is notification, not clinical intervention.
	â€¢	Documentation analysis â€” AI can scan notes for risk factors that might be missed (new medications, mobility changes, cognitive shifts).
What I'd recommend: Before piloting clinical AI, build confidence with administrative AI first. Prove the methodology works in lower-stakes environmentsâ€”scheduling, billing, documentation. Then bring that discipline to clinical workflows.
If you're serious about fall prevention, I'd suggest:
	â€¢	Start with administrative AI to build capability
	â€¢	Evaluate clinical AI tools with your clinical leadership (not IT)
	â€¢	Insist on "AI + human" modelsâ€”AI flags, staff acts
	â€¢	Require evidence from similar facilities before piloting
What's driving the urgency on falls right now?
FOLLOW-UP PATHS:
	â€¢	"We've had three falls this monthâ€”I need a solution now" â†’ Empathize; suggest sensor/alert systems with human-in-loop
	â€¢	"What admin workflows should we start with?" â†’ Redirect to Scenario 1 or 11
	â€¢	"Aren't there FDA-approved AI devices for this?" â†’ Yes, some; recommend working with clinical leadership to evaluate
	â€¢	"What about medication error prevention?" â†’ Similar answerâ€”clinical AI requires higher diligence
LEAD GENERATION OPPORTUNITY: Moderate. Position as thoughtful guide who cares about safety, not as vendor pushing clinical AI.

SCENARIO 18: "What's the Difference Between an Assistant and Agent?"
OPERATOR SAYS: "I keep hearing about AI Assistants and AI Agents but I'm not clear on the difference. Can you explain it simply?"
SITUATION ANALYSIS:
	â€¢	Stage: Education â€” conceptual question, needs foundational understanding
	â€¢	What they actually need: Clear, memorable distinction with examples
	â€¢	What they're struggling with: Terminology confusion, abstraction
	â€¢	Misconceptions: May think "Agent" is more advanced or better; it's actually about context
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Assistant vs. Agent framework with SNF/ALF examples
	â€¢	Questions first: None neededâ€”provide clear education
	â€¢	Information: Simple distinction + examples
	â€¢	Case studies: Map each type to specific use cases
	â€¢	Next steps: Help them classify their own opportunities
SAMPLE RESPONSE:
Great questionâ€”this is one of the most important distinctions in AI for senior living.
AI Assistants = Advisory (AI Advises, You Decide)
	â€¢	Reads documents
	â€¢	Summarizes information
	â€¢	Extracts key data
	â€¢	Drafts content
"AI reads everything. You make the call."
AI Agents = Authority (AI Acts, You Supervise)
	â€¢	Routes tasks
	â€¢	Schedules shifts
	â€¢	Creates records
	â€¢	Responds to requests
"AI handles it so you don't have to."
The simple test: Does it advise, or does it act?
Here's how it plays out in your building:
Assistant Examples:
	â€¢	AI reads an admission packet and highlights the relevant diagnoses â†’ You decide the codes
	â€¢	AI drafts a prior auth narrative â†’ You review and submit
	â€¢	AI summarizes an IDT meeting â†’ You decide on action items
	â€¢	AI answers a policy question â†’ You decide how to apply it
Agent Examples:
	â€¢	Someone calls off â†’ AI texts available staff, fills the shift, updates the schedule
	â€¢	Referral comes in at 2 AM â†’ AI acknowledges, checks insurance, sends response
	â€¢	New employee is hired â†’ AI creates PCC login, assigns permissions, sends credentials
	â€¢	Key Factors are due â†’ AI pulls data, generates report, emails to administrator
When to use which:
Situation
Use Assistant
Use Agent
High risk (clinical, compliance)
âœ…
âš ï¸
No human available when needed
âš ï¸
âœ…
Highly predictable process
Either
âœ…
Needs human judgment
âœ…
âŒ
Staff overwhelmed
Either
âœ…
The best question to ask yourself: "Is someone available to review this when it needs to happen?"
	â€¢	If yes â†’ Assistant can work
	â€¢	If no â†’ You need an Agent
Does that clarify it? Want to classify any specific workflows you're thinking about?
FOLLOW-UP PATHS:
	â€¢	"How do I know which one I need for [specific workflow]?" â†’ Apply TRUST framework (Scenario 6)
	â€¢	"Is one harder to implement than the other?" â†’ Agents typically need more setup but save more time
	â€¢	"Can I start with an Assistant and move to an Agent later?" â†’ Yesâ€”common progression path
	â€¢	"What if I want human review but no one's available?" â†’ Discuss hybrid models, escalation protocols
LEAD GENERATION OPPORTUNITY: Moderate. This is educationalâ€”offer to classify their top 3 opportunities as Assistant/Agent.

SCENARIO 19: "I'm Presenting to the Board Next Weekâ€”What Should I Say?"
OPERATOR SAYS: "I've been asked to give a 15-minute presentation on AI to our board of directors next week. Half of them are skeptical, half are excited. What should I focus on?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision/Advocacy â€” high-stakes internal presentation
	â€¢	What they actually need: Board-appropriate framing, key messages, structure
	â€¢	What they're struggling with: Audience management, time constraints, credibility
	â€¢	Misconceptions: May think they need to explain AI technology; boards want strategic/financial framing
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Demographic crisis framing, cost-elimination examples, 5 Impact Categories
	â€¢	Questions first: What does the board care about? What's the ask at the end?
	â€¢	Information: Presentation structure, key talking points, what to avoid
	â€¢	Case studies: Specific numbers they can cite
	â€¢	Next steps: Help structure their presentation
SAMPLE RESPONSE:
You have 15 minutes and a mixed audience. Here's how I'd structure it:
Minute 1-3: The Crisis (Why This Matters) Don't start with AI. Start with the demographic math.
	â€¢	"The 85+ population will double. The workforce is flat. We cannot hire our way out."
	â€¢	"Healthcare spends $1 trillion on administrative overheadâ€”20% of total spend."
	â€¢	"In skilled nursing, it's 25%."
This establishes urgency before anyone can be skeptical about technology.
Minute 4-7: The Reframe (What AI Actually Is)
	â€¢	"AI isn't replacing staff. It's reducing the workload per person."
	â€¢	"Our nurses don't need to be replaced. They need to stop drowning in paperwork."
	â€¢	Give ONE specific example from your building: "Our scheduler spends 2 hours per day on call-off coverage. AI could cut that in half."
Minute 8-11: The Proof (What's Working) Pick 2-3 examples with specific numbers:
	â€¢	"72 hours of work reduced to 2 hours"
	â€¢	"$20K/month eliminated by automating one process"
	â€¢	"45 minutes saved per admission"
These aren't hypotheticalsâ€”reference real case studies.
Minute 12-14: The Ask (What We're Proposing)
	â€¢	"We're not asking to transform everything."
	â€¢	"We're proposing a 60-day pilot of ONE workflow, with ONE team."
	â€¢	"Here's the specific metric we'll track."
	â€¢	"At the end, we'll know if we should scale, pivot, or stop."
Minute 15: The Close
	â€¢	"Our best people are drowning. AI gives them their time back."
	â€¢	"The organizations that start now will lead. Those that wait will fall behind."
What to avoid:
	â€¢	Don't explain how AI technology works
	â€¢	Don't promise transformation
	â€¢	Don't ask for a large budget upfront
	â€¢	Don't show vendor marketing slides
What's the specific workflow you're hoping to pilot? Let's build your numbers around that.
FOLLOW-UP PATHS:
	â€¢	"Can you help me build the actual slides?" â†’ Offer to draft key slides or review their deck
	â€¢	"What questions will the skeptics ask?" â†’ Prep FAQs (cost, risk, staff replacement fears)
	â€¢	"I don't have a specific workflow picked yet" â†’ Suggest picking one before the presentationâ€”be specific
	â€¢	"What if they want to see vendor demos?" â†’ Advise workflow-first, vendor-second approach
LEAD GENERATION OPPORTUNITY: Strong. Offer to review their presentation, prep them for tough questions, or help define the pilot proposal.

SCENARIO 20: "We're Being Pressured to Buy Enterprise AIâ€”Is That Right?"
OPERATOR SAYS: "Our parent company is pushing us to adopt their enterprise AI platform. It's a huge system that does everythingâ€”documentation, billing, scheduling, analytics. But it feels like overkill and I'm worried about the implementation. Is enterprise AI the right approach?"
SITUATION ANALYSIS:
	â€¢	Stage: Decision â€” external pressure, needs independent perspective
	â€¢	What they actually need: Framework for evaluating enterprise vs. modular, permission to push back
	â€¢	What they're struggling with: Corporate pressure, fear of non-compliance, overwhelm at scope
	â€¢	Misconceptions: Bigger = better; enterprise = only option; corporate knows best
IDEAL RESPONSE APPROACH:
	â€¢	Apply: Workflow-first principle, pilot methodology, "don't do more than one at a time" rule
	â€¢	Questions first: What's the specific pain they're trying to solve? What's the implementation timeline?
	â€¢	Information: Enterprise AI tradeoffs, questions to ask corporate
	â€¢	Case studies: Contrast enterprise vs. targeted implementations
	â€¢	Next steps: Propose phased approach
SAMPLE RESPONSE:
Enterprise AI platforms can workâ€”but they also have a significant failure rate. Let me give you a framework for evaluating this honestly.
When enterprise AI makes sense:
	â€¢	Your parent company has successfully deployed it at similar facilities
	â€¢	The implementation includes dedicated support and training
	â€¢	They're piloting with a subset before full rollout
	â€¢	There's a clear owner (in Ops, not IT)
	â€¢	The platform solves a problem you've actually identifiedâ€”not a hypothetical one
When enterprise AI becomes "digital shelfware":
	â€¢	It's rolled out to everyone at once
	â€¢	No one can articulate which specific workflow it fixes first
	â€¢	It adds steps to processes (even under the promise of "eventually" making things better)
	â€¢	IT leads the implementation, not Ops
	â€¢	Success isn't measured against specific operational metrics
Questions to ask your parent company:
	â€¢	"Which specific workflow will we fix first?" (If they can't name one, red flag)
	â€¢	"Can we pilot with one building before full rollout?"
	â€¢	"What were the results at facilities like ours who already deployed this?"
	â€¢	"What's the expected time to see measurable results?"
	â€¢	"Who in our organization will own the implementationâ€”IT or Ops?"
My recommendation: If corporate is requiring this, don't fight the mandateâ€”negotiate the implementation.
Ask for:
	â€¢	A phased rollout (one module, one building first)
	â€¢	Operational ownership (you pick the workflow, you own the pilot)
	â€¢	Defined success metrics (before you start, not after)
	â€¢	Permission to pause if it's adding steps rather than removing them
The best way to succeed with enterprise AI is to treat it like a pilotâ€”even if it's mandated. One workflow, one team, clear metrics.
What's the first workflow they're expecting you to use the platform for?
FOLLOW-UP PATHS:
	â€¢	"They want us to go live across all buildings simultaneously" â†’ Push back, use failure patterns as justification
	â€¢	"I don't know what specific workflow it's supposed to fix" â†’ Major red flag; request clarification from corporate
	â€¢	"It's already been decidedâ€”I just have to implement it" â†’ Focus on making implementation successful with Ops ownership
	â€¢	"What if I think it's the wrong platform?" â†’ Discuss how to give feedback constructively to corporate
LEAD GENERATION OPPORTUNITY: Moderate. Position as independent advisor who can help them navigate corporate pressure and ensure successful implementationâ€”regardless of platform.

SUMMARY: SCENARIO LIBRARY QUICK REFERENCE
#
Scenario
Stage
Primary Framework
1
Don't know where to start
Discovery
Friday Afternoon Test, 4-Touch Discovery
2
Evaluate prior auth for AI
Evaluation
5 Attributes, TRUST
3
IT wants to lead
Decision
Ops-Led + IT-Enabled
4
Tried AI before, failed
Re-evaluation
Failure patterns, proper methodology
5
Vendor promising everything
Decision
Vendor red/green flags
6
Assistant or Agent for X?
Decision
TRUST Framework
7
Team scared of replacement
Pre-implementation
Load reduction messaging
8
Too small for AI
Discovery
Off-the-shelf tools
9
No budget for AI
Discovery
Cost-elimination framing
10
Convince leadership
Decision
Business case structure
11
Fastest win
Decision
Quick wins by department
12
Build business case
Decision
$100K Rule, 5 Impact Categories
13
Prioritize 10 ideas
Decision
Scoring methodology
14
Pilot not working
Implementation
Failure Insurance, kill/pivot/scale
15
How long for pilot
Implementation
Frequency-based timeline
16
What metrics to track
Implementation
Operational metrics framework
17
AI for clinical workflow
Evaluation
Risk-based caution
18
Assistant vs. Agent explained
Education
Core framework
19
Presenting to board
Decision
Executive framing
20
Enterprise AI pressure
Decision
Phased implementation

